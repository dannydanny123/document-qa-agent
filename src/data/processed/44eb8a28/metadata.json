{
    "doc_id": "44eb8a28",
    "title": "Introduction",
    "abstract": "",
    "sections": [],
    "chunks": [
        {
            "chunk_id": "44eb8a28_p1_0",
            "text": "Attention Is All You Need\nIntroduction",
            "start_pos": 0,
            "metadata": {
                "doc_id": "44eb8a28",
                "page": 1,
                "section": "Introduction"
            }
        },
        {
            "chunk_id": "44eb8a28_p1_1",
            "text": "In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V . We compute the matrix of outputs as: Attention(Q,K, V ) = softmax(QKT√dk)V The two most commonly used attention functions are additive attention [2], and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of 1 √dk . With a single attention head, averaging inhibits this. MultiHead(Q,K, V ) = Concat(head1, ..., headh)WO where headi = Attention(QWQ i ,KWK i , VWV i ) Where the projections are parameter matricesWQ i ∈ Rdmodel×dk ,WK i ∈ Rdmodel×dk ,WV i ∈ Rdmodel×dv and WO ∈ Rhdv×dmodel . In this work we employ h = 8 parallel attention layers, or heads. For each of these we use dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full",
            "start_pos": 39,
            "metadata": {
                "doc_id": "44eb8a28",
                "page": 1,
                "section": "Introduction"
            }
        },
        {
            "chunk_id": "44eb8a28_p1_2",
            "text": "layers, or heads. For each of these we use dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality",
            "start_pos": 834,
            "metadata": {
                "doc_id": "44eb8a28",
                "page": 1,
                "section": "Introduction"
            }
        },
        {
            "chunk_id": "44eb8a28_p1_3",
            "text": "ReLU activation in between. FFN(x) = max(0,xW1 + b1)W2 + b2 (2) While the linear transformations are the same across different positions\nIn this work, we use sine and cosine functions of different frequencies: PE(pos,2i) = sin(pos/100002i/dmodel ) PE(pos,2i+1) = cos(pos/100002i/dmodel ) where pos is the position and i is the dimension. That is, each dimension of the positional encoding\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning rate over the course of training, according to the formula: lrate = d−0.5 model · min(step_num−0.5, step_num · warmup_steps−1.5) (3) This corresponds to increasing the learning rate linearly for the first warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup_steps = 4000.",
            "start_pos": 1043,
            "metadata": {
                "doc_id": "44eb8a28",
                "page": 1,
                "section": "Introduction"
            }
        }
    ],
    "tables": [],
    "figures": [
        {
            "id": "fig_2_0",
            "page": 2,
            "bbox": [
                24.0,
                30.75,
                165.75,
                239.25
            ],
            "path": "src\\data\\assets\\44eb8a28\\fig_page2_0.png",
            "caption": ""
        },
        {
            "id": "fig_2_1",
            "page": 2,
            "bbox": [
                229.82965087890625,
                10.5,
                493.82965087890625,
                239.25
            ],
            "path": "src\\data\\assets\\44eb8a28\\fig_page2_1.png",
            "caption": ""
        }
    ],
    "equations": [],
    "full_text_preview": "Attention Is All You Need\n\nIntroduction\n\nIn practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V . We compute the matrix of outputs as: Attention(Q,K, V ) = softmax(QKT√dk)V The two most commonly used attention functions are additive attention [2], and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of 1 √dk . With a single attention head, averaging inhibits this. MultiHead(Q,K, V ) = Concat(head1, ..., headh)WO where headi = Attention(QWQ i ,KWK i , VWV i ) Where the projections are parameter matricesWQ i ∈ Rdmodel×dk ,WK i ∈ Rdmodel×dk ,WV i ∈ Rdmodel×dv and WO ∈ Rhdv×dmodel . In this work we employ h = 8 parallel attention layers, or heads. For each of these we use dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality\n\nReLU activation in between. FFN(x) = max(0,xW1 + b1)W2 + b2 (2) While the linear transformations are the same across different positions\n\nIn this work, we use sine and cosine functions of different frequencies: PE(pos,2i) = sin(pos/100002i/dmodel ) PE(pos,2i+1) = cos(pos/100002i/dmodel ) where pos is the position and i is the dimension. That is, each dimension of the positional encoding\n\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning rate over the course of training, according to the formula: lrate = d−0.5 model · min(step_num−0.5, step_num · warmup_steps−1.5) (3) This corresponds to increasing the learning rate linearly for the first warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup_steps = 4000."
}