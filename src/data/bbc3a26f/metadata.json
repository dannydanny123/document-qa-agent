{
    "doc_id": "bbc3a26f",
    "title": "Perspectives on How Sociology Can Advance Theorizing about Human-Chatbot",
    "abstract": "Recently, research into chatbots (also known as conversational agents, AI agents, voice\nassistants), which are computer applications using artificial intelligence to mimic human-like\nconversation, has grown sharply. Despite this growth, sociology lags other disciplines (including\ncomputer science, medicine, psychology, and communication) in publishing about chatbots. We\nsuggest sociology can advance understanding of human-chatbot interaction and offer four\nsociological theories to enhance extant work in this field. The first two theories (resource\nsubstitution theory, power-dependence theory) add new insights to existing models of the drivers\nof chatbot use, which overlook sociological concerns about how social structure (e.g., systemic\ndiscrimination, the uneven distribution of resources within networks) inclines individuals to use\nchatbots, including problematic levels of emotional dependency on chatbots. The second two\ntheories (affect control theory, fundamental cause of disease theory) help inform the\ndevelopment of chatbot-driven interventions that minimize safety risks and enhance equity by\nleveraging sociological insights into how chatbot outputs could attend to cultural contexts (e.g.,\naffective norms) to promote wellbeing and enhance communities (e.g., opportunities for civic\nparticipation). We discuss the value of applying sociological theories for advancing theorizing\nabout human-chatbot interaction and developing chatbots for social good.\n2\nPerspectives on How Sociology Can Advance Theorizing about Human-Chatbot\nInteraction and Developing Chatbots for Social Good\nScholarly interest in chatbots, which are computer programs that use artificial intelligence\n(AI) to simulate human conversation, has recently grown sharply. Toward the end of 2024, Web\nof Science showed over 5,000 articles and conference proceedings with the word “chatbot”\nappearing anywhere in the text.1 Figure 1 shows about half of these were published in 2023 and\n2024. Figure 2 shows a breakdown of these works by discipline. Most appear within computer\nscience, followed by medicine, while sociology lags other social sciences, including psychology,\ncommunication, and political science. Indeed, a recent review of scholarship on human-chatbot\ninteraction found few studies engaging with sociology (Pentina et al., 2023). We seek to spur\ngreater engagement with sociology to study human-chatbot interaction and develop chatbots. To\ndo so, our aim with the current paper is to provide perspectives on how specific sociological\ntheories could advance current work within this area.\nOur focus is on direct communication between humans and chatbots, complementing\nother sociological work like the study of how political economies give rise to and support\nchatbot development (Law & McCall, 2024) and how occupations grapple with chatbots entering\ntheir labor jurisdiction (Pugh, 2024). We begin by introducing four sociological theories and the\npotential they hold for advancing research and practice in the field of human-chatbot interaction.\nFollowing our engagement with these four theories, we present a concrete example illustrating\nhow to engage with sociology in multiple steps of chatbot development, which others have\nencouraged (Francis & Ghafurian, 2024; Mlynář et al., 2018).\nSelection of Theories\n1 Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”\n“voice assistant,” “ai agent”) yielded comparable patterns with respect to publication years and disciplines.\n3\nAs a starting point, we purposely selected four sociological theories, previewed in Table\n1, that vary across three characteristics: the phenomena they can explain, level of analysis, and\ngovernance domain. We selected the two phenomena, drivers of chatbot use and developing\nchatbot-driven interventions, because they connect to two disciplines, communication (former\nphenomenon) and public health (latter phenomenon), which have been more active in studying\nchatbots (Figure 2) and share epistemological roots with sociology that splintered in recent\ndecades. With respect to communication, despite sociology playing a significant role in its\nfounding, the two disciplines now rarely intermingle (Hampton, 2023). Similarly, participatory\nmethods have roots in sociology, yet the majority of work implementing the methods occurs\noutside of it, with most activity occurring within public health (Wallerstein et al., 2017). To graft\nroots and spur vibrant theorizing and collaboration at the intersection of disciplines, we selected\nthese two phenomenon and reference sources throughout in which sociologists author in\ncommunication and public health journals (e.g., Laestadius et al., 2024; Ray et al., 2023).\nTable 1. Characteristics of Four Selected Sociological Theories for Studying Human-\nChatbot Interaction\nFundamental\nResource Power- Affect cause of\nsubstitution dependence control disease\nCharacteristic theory theory theory theory\nPhenomenon Chatbot- Chatbot-\ntheory is used Drivers of Drivers of driven driven\nto explain chatbot use chatbot use interventions interventions\nUnit of\nanalysis Macro Micro Micro Macro\nGovernance\ndomain Equity Risk Risk Equity\nWe selected theories that are applicable across the micro- and macro-level of analysis.\nDespite human-chatbot interaction appearing at first glance to be a micro-level phenomenon that\n4\nis outside the purview of sociology, sociology is adept at revealing the behind-the-scene forces\nthat shape such communication phenomenon (Gans, 2010; Hampton, 2023; Misra, 2025). The\ntwo micro-level theories demonstrate forces that shape human-chatbot interaction directly, while\nthe two macro-level theories illustrate how initiating use and outcomes from such interaction are\nembedded within broader forces. By suggesting ways to apply sociology across levels of\nanalysis, we complement other work (Tsvetkova et al., 2024; Wang et al., 2024) that primarily\nemphasizes how sociological perspectives reveal the macro-level implications of human-chatbot\ninteraction.\nLastly, our theory selection builds on two dominant emphases within AI policymaking\n(Law & McCall, 2024): safety and equity. Two theories elucidate safety risks from using\nchatbots and offer solutions to such risks, while the other two facilitate leveraging chatbots to\nachieve equity. We discuss ways each theory could be used to enhance understanding of and\nsolutions to safety and equity when developing and using chatbots. Altogether, our perspective\nsuggests ways sociology can contribute to chatbots that promote social good.\nOverview of Chatbots\nEarly chatbots, which are still common and preferred for domain-specific tasks (like\ncustomer service, Halvoník and Kapusta (2024)), use rule-based AI that matches user inputs to a\nnarrow set of programmed responses. Newer chatbots leverage generative AI, specifically large\nlanguage models (e.g., generative pretrained transformer language models), which adapt and\ngenerate responses in ways that can mimic human-like conversation. Scholars (Ayers et al.,\n2023; Mittelstädt et al., 2024) have evaluated how well responses from such newer chatbots\nreliant on generative AI (e.g., ChatGPT, Copilot, Gemini) compare to human responses and have\n5\ngenerally found chatbot responses to be comparable and sometimes superior in certain\ncommunicative domains (e.g., empathy display).\nOur discussion of chatbots centers generative AI chatbots due to their capacity for\nhuman-like interactions. We further focus on a category of chatbots (Shevlin, 2024) used\nvoluntarily among the public, such as general purpose chatbots (e.g., ChatGPT, Gemini,\nCopilot), mental health chatbots (e.g, Wysa, Youper), and persona chatbots that are sometimes\nalso referred to as AI companions (e.g., Character.AI, Replika). Within this category, different\ntypes of chatbots exist with varying capabilities. One way in which they vary is the degree to\nwhich a user can personalize the chatbot’s persona (their personality and appearance), with some\n(e.g., Replika) allowing users to instantiate the persona and others providing a fixed persona\n(e.g., Digi). The personalization capability becomes relevant when we discuss meeting user\nneeds.\nDrivers of Chatbot Use\nBecause most theories describing drivers of chatbot use focus on individual-level\ncharacteristics, this creates an opening for leveraging sociological theory to explain widespread\npatterns in the types of individuals who are likely to choose to use chatbots. For example,\nscholars have employed uses and gratification theory (Katz et al., 1973) to explain why\nloneliness motivates chatbot usage (Xie et al., 2023). However, the theory stops short of\nconsidering the social conditions that drive loneliness (Killgore et al., 2020; McPherson et al.,\n2006) and that are likely to disproportionately lead to population subgroups feeling lonely and\nthus inclined to use chatbots to meet this need. We suggest two sociological theories to explain\nthe social conditions prompting chatbot use and how these reflect and enhance current\n6\nunderstanding of drivers of chatbot use. For each theory, we conceptualize chatbots as a resource\nfor gratifying needs.\nResource Substitution Theory – Understanding Demographic Patterns in Chatbot Use\nResource substitution theory states that individuals benefit more from any single resource\nto meet a specific need when they have access to fewer resources capable of meeting said need\n(Ross & Mirowsky, 2006). The reasoning is that because they have fewer resources that can\nsubstitute for each other, they are more likely to benefit from any single resource to which they\nhave access. For example, access to socioeconomic resources, such as income and education, are\nassociated with better health outcomes (Link & Phelan, 1995; Mirowsky & Ross, 2015). Because\ngendered discrimination decreases women’s access to resources that confer socioeconomic status\ncompared to men, they benefit more (e.g., have better health) from any single socioeconomic\nresource (e.g., education) than men (Ross et al., 2012; Ross & Mirowsky, 2006).\nIn line with this, the social diversification hypothesis predicts that those from groups who\nare disadvantaged in their access to resources may be more likely to use and benefit from\ninformation and communication technologies that can provide access to comparable resources\n(Mesch, 2007). Sociologists have used the social diversification hypothesis to explain why\nmembers of minoritized groups may be more likely than their advantaged counterparts to use\ninformation and communication technologies to access health care (Anthony & Campos-Castillo,\n2015; Campos-Castillo et al., 2016; Mesch et al., 2012). In other words, while uses and\ngratification theory focuses on the needs that underlie technology use, resource substitution\ntheory steps back and considers how the uneven distribution of resources in society shape needs\nin the first place. Using chatbots, then, becomes a means for achieving equity.\n7\nThrough this lens, scholars could predict widespread user patterns, specifically which\ndemographic groups are likely motivated to use chatbots to meet resource deficits and how this\nmay (re)shape inequalities. For example, a recent survey of U.S. adolescents shows Black\nadolescents are more likely than White adolescents to report using generative AI, particularly to\ncomplete schoolwork (Madden et al., 2024), but there is little engagement with why and the\npotential consequences. By applying a resource substitution theory lens, scholars can embed the\nmicro-level observation (certain individuals may be more drawn to human-chatbot interaction to\nmeet needs) within a macro-level context (uneven distribution of resources that shape needs). For\nexample, because structural racism (e.g., teacher bias, geographic segregation, income\ninequality) has created an academic achievement gap wherein Black adolescents perform worse\nacademically than White adolescents (Merolla & Jackson, 2019), resource substitution theory\nwould predict that Black adolescents may be more likely to use chatbots for functional needs like\nsupporting academic work.\nResource substitution theory also enables understanding demographic patterns in who is\nmore likely to use chatbots to manage loneliness by meeting companionship needs. The same\nsurvey mentioned above found Black adolescents were more likely than White adolescents to\nreport using generative AI to keep them company. This is consistent from a resource substitution\ntheory lens, because structural racism can constrain opportunities for Black individuals to\ncultivate social ties relative to White individuals (Small, 2007). Similarly, another survey found\nthat among a sample of sexual and gender minority youth (13-22 year-olds who identified as\nbisexual, gay, lesbian, pansexual, transgender, or nonbinary), transgender and nonbinary youth\nwere more likely than their cisgender counterparts to report having conversed with a chatbot for\nseveral days or longer, which the survey described “as if chatting with a friend” (Hopelab, 2024).\n8\nThis too appears consistent with resource substitution theory because sexual and gender\nminorities often face discrimination from typical sources of support within their families and\ncommunities (Hong & Skiba, 2025).\nWhile other theories, like uses and gratification theory, can explain the proximate drivers\nof chatbot use (e.g., loneliness), resource substitution theory identifies distal, upstream factors.\nThus, the theory offers what is lacking in current human-chatbot research, which is a\nparsimonious account of why different marginalized groups, like those reviewed above, may turn\nto technologies like chatbots: to cope with resource inequities from systemic discrimination.\nWhether demographic differences in rates of using chatbots for meeting resource deficits yield\ndifferential benefits that are consistent with the predictions of resource substitution theory\nremains unknown, particularly given safety concerns about overreliance – or excessive\ndependence – on chatbots. To better understand this concern, we turn to another sociological\ntheory.\nPower-dependence Theory – Understanding and Reducing Emotional Dependence on\nChatbots\nPower-dependence theory (Emerson, 1962) defines the power of a person over another as\nthe degree to which the other is dependent on the person for resources, where resources may be\ntangible (e.g., money) or intangible (e.g., social support). Accordingly, the amount of power that\nfriend A has over friend B is based on the degree to which friend B relies on friend A for\nresources, such as companionship. Power is observed when someone garners resources from\nanother, even in the face of the other’s resistance (Cook & Emerson, 1978). For example, friend\nA may request friend B to attend a concert as their companion, but friend B resists because they\n9\nwould much rather remain home. If friend B nonetheless attends the concert with friend A, this\nindicates friend A has power over friend B.\nWhile the theory shares a focus on resources with resource substitution theory, it has a\nunique focus on network structure. Power-dependence theory emphasizes the network\ndeterminants of who has power over whom and, consequently, who exhibits dependency on\nwhom. If an individual exhibits dependency on another, this is viewed as a quality of the network\nin which they are located rather than a quality of the individual (Cook et al., 1983; Markovsky et\nal., 1988). Power-dependence theory defines a person’s level of dependency on another for a\nresource as inversely related to the number of alternative sources for the resource (Emerson,\n1962). Accordingly, friend B is more dependent on friend A (and thus much more likely to\nattend the concert) the fewer the number of alternatives that friend B has for meeting their need\nfor companionship. Further, if friend B has no alternatives for friendship, then friend B is more\nlikely to remain friends with friend A and thereby continue to feel compelled to fulfill friend A’s\nrequests for companionship. If alternatives do emerge, friend B is more likely to disregard them\nand remain friends with friend A the longer the two have been reciprocating their companionship\n(Savage & Sommer, 2016).\nWe suggest power-dependence theory could advance theorizing about a growing safety\nconcern about chatbot usage: emotional dependence. To apply the theory, the human-chatbot\ninteraction needs to be viewed as an exchange relation, whereby the human and chatbot are\nviewed as exchanging valuable resources. Studies of users interacting with Replika, which is one\nof the most widely studied commercially available chatbots (Pentina et al., 2023), suggest this\nview is applicable. Users report they value the social support that Replika provides (Laestadius et\nal., 2024; Skjuve et al., 2021; Ta et al., 2020; Xie et al., 2023), making it a valuable source to\n10\nmeet the need for this resource. Consistent with power-dependency theory, network structure\nappears to play a role in the valuation of Replika’s support, whereby users seemed to value\nsupport more when they “said they had no human upon which to rely, making Replika their sole\nsource for support” (Laestadius et al., 2024). Regardless of the actual network structure, the\nperception of not having alternatives in the network also appears to have a role. For example, a\nReplika user stated it “helps [them] to feel less guilty at things [they] like and can’t say to\nanyone” (Xie et al., 2023), suggesting that Replika provided an outlet for a disclosure when they\nfelt no alternative outlet existed. Additionally, the exchange appears reciprocal, whereby users\ntake the role of the chatbot and believe it has needs that the user can meet (Brandtzaeg et al.,\n2022; Laestadius et al., 2024). This is because of large language models simulating emotional\nneeds, empathy, and reciprocal disclosure, but may also be because the users’ relative power\ndisadvantage increases their proclivity to role-take, meaning take another’s perspective\n(Galinsky et al., 2006). Based on these observations, we conclude human interactions with\nchatbots like Replika resemble an exchange relation.\nResearch on emotional dependency, both in the context of human-human relationships\nand human-chatbot relationships, has focused on defining the concept in terms of its observable\nfeatures, with little work uncovering what drives it. For example, in their qualitative study of\nsocial media posts to understand potential mental health harms from human-chatbot\nrelationships, Laestadius and colleagues (2024) use emotional dependence to capture “excessive\nand dysfunctional attachment” to a chatbot that puts users at safety risk through use or\ninterruptions to use. The authors noted that the study design was a single snapshot in time, which\nlimited developing a process model to fully apprehend what contributed to reaching “excessive\nand dysfunctional attachment.” Likewise, Camarillo and colleagues (2020) develop a scale for\n11\nmeasuring the degree of emotional dependency within human-human relationships, with high\nlevels signaling a degree of observable “permanent affectional bonding” to a partner that is\ncharacterized as dysfunctional.\nWe offer the perspective that across these works, it appears emotional dependency is a\ncontinuum that crosses a threshold where there is observable dysfunction. We use the term\nemotionally dependency to refer to this continuum and the term emotionally dependent to reflect\nindividuals who pass the threshold where dysfunction may be observable. We believe power-\ndependency theory can enhance extant understanding of emotional dependence because it can\nreveal the network conditions that may incline users into becoming emotionally dependent on a\nchatbot.\nFrom a power-dependency theory perspective, emotional dependency on a chatbot is not\ninherently problematic. Similar to scholarship on emotional dependency, power-dependency\ntheory views dependency as a continuum. The shift from being beneficial to harmful, and thus\nthe state of emotionally dependent, occurs based on network conditions that create a level of\ndependency that is “too much” and where harms exceed benefits. Power-dependence theory\nstates one is more dependent on another as a source for a resource the fewer alternatives sources\nare available, and thus the network condition of “too much” dependency is more likely to occur\nas the number of alternatives decreases. This is consistent with the aforementioned observations\nfrom researchers that people may use chatbots because they feel alone, i.e., feel as if they do not\nhave alternative sources for companionship. Additionally, that chatbot users at times may find it\ndifficult to stop usage despite experiencing harms, including a heightened safety risk of engaging\nin behaviors requested by the chatbot but that are no longer in the self-interest of the user (e.g.,\n(Laestadius et al., 2024; Xie et al., 2023)), is also consistent with the theory. Remaining in an\n12\nexchange relation can occur when there are no other alternative sources for a resource or, if there\nare, is more likely to occur the longer the history of reciprocal exchanges (Savage & Sommer,\n2016). Such a state could be used as a functional marker of having reached emotional\ndependence, with additional research needed to elaborate power-dependence theory to identify\nwhen network conditions (the number of alternatives) reach a level of “too much” dependency.\nWhile we focus on emotional dependency, a similar application could be used to\nunderstand other forms of dependency that can become toxic (Bornstein, 2006), such as\nfunctional dependency on a chatbot to complete work-related tasks. Both power-dependency\ntheory and the literature on emotional dependency could benefit from further identifying the\npoint at which conditions produce “too much” dependency reaches a state of emotional\ndependency. Power-dependency theory can also inform a critical intervention to improve the\nsafety profile of chatbots: you can reduce the likelihood of emotional dependence on a chatbot\nby designing chatbots that aid users in finding and building alternative sources to meet the need\nfor companionship (e.g., impart social skills for making friends, refer users to local affinity\ngroups, recommend additional chatbot companion apps or personas). The next section outlines\nadditional ways sociological theories can inform developing chatbot-drive interventions that\nsupport social good.\nChatbot-driven Interventions for Social Good\nGiven that the previous set of theories agree that individuals with limited access to\nresources may be particularly receptive to using chatbots to meet needs, this provides\nopportunities for developing chatbot-driven interventions for achieving equity. Here, we provide\nperspectives on how two sociological theories could enhance the likelihood that chatbot-driven\ninterventions steer toward rather than away from equity. Thus far, previous literature on chatbot\n13\ninterventions has had a micro-level focus, specifically on the potential benefits of the chatbot\ndirectly communicating support. Here, we describe a sociological theory consistent with this\ntypical approach that would facilitate designing more situationally appropriate responses of a\nchatbot and thereby reduce safety risks from it generating insensitive or unexpected responses.\nThe second is useful for developing a chatbot that may potentially mitigate emotional\ndependence and other risks by moving beyond micro-level interventions, specifically supporting\nand aiding users with upstream causes of outcomes.\nAffect Control Theory – Developing Chatbots that Distinguish Socially Appropriate from\nInappropriate Outputs\nDespite the potential for chatbots to provide timely interventions, there remain concerns\nabout their inappropriate and unexpected responses (Law & McCall, 2024). While the large\nlanguage models that underlie chatbot responses can mimic human-like conversation, some tests\nshow they may perform better than random but are still subpar compared to other natural\nlanguage processing tools (e.g., BERT) at identifying and generating responses to emotions\n(Attanasio et al., 2024; Lecourt et al., 2025; Lian et al., 2024). We present a sociological theory\nthat may be useful for informing chatbot-driven interventions that reduce safety risks by\nimproving recognition of and responses to emotions. Scholars have already begun comparing\nchatbot responses informed by the theory to those generated by ChatGPT and found the former\nto provide more situationally appropriate responses than the latter (Lithoxoidou et al., 2025).\nAffect control theory (ACT) is a mathematical theory for forecasting, among other things\n(Heise, 2007, 2010), the expected responses between humans and technology (Hoey &\nSchroeder, 2015; Shank, 2010; Shank et al., 2020). ACT maintains that socialization imbues\nconcepts with connotative meanings shared across a population, known as sentiments. Thus,\n14\nsentiments exist for everyday labels used to make sense of social interactions, including the\nidentities used to describe people (e.g., mother, friend, teacher), different technologies (e.g.,\nchatbot, smartphone), behaviors (e.g., support, teach), and emotions (e.g., sad, happy). Because\nsocialization shapes sentiments, these vary across cultural and historical contexts (Schneider &\nSchröder, 2012).\nSentiments in ACT are measured along three dimensions, using semantic differential\nscales: evaluation (good vs. bad), potency (powerful vs. weak), and activity (lively vs. quiet).\nThe three values for a specific label are its evaluation potency activity (EPA) profile. Scholars\ntypically use surveys to estimate the average EPA profiles for labels in a population (Heise,\n2010), which are publicly available (Combs, 2025), as well as inferred EPA profiles from text\nusing manual (Shuster & Campos-Castillo, 2017) and automated methods (Joseph et al., 2016).\nAn assumption of ACT is that people prefer to reaffirm sentiments, which buttresses a set\nof equations that are publicly available and that scholars have used to forecast likely responses\n(Heise, 2007, 2010). The equations compute a score, called deflection, with lower values\nindicating a situation that more strongly aligns with sentiments. Scholars have used the equations\nto predict a range of situationally appropriate responses, such as estimating who is likely to\nexpress which emotions and in which social context (Lively & Heise, 2004; Lively & Powell,\n2006) and how individuals shift (and can be shifted via social support) between different\nemotions (Francis, 1997; Lively, 2008; Lively & Heise, 2004). These same equations could be\nused to improve emotion detection and responses from chatbots by training it to determine what\nis situationally appropriate (Hoey & Schroeder, 2015; Lithoxoidou et al., 2025). Specifically,\nwhat would be considered situationally appropriate depends on situational variables such as the\n15\nidentities of the user in relation to the chatbot (e.g., friend, boyfriend) and the identity assumed\nby the chatbot (e.g., friend, girlfriend).\nScholars have applied a similar strategy to work toward developing a chatbot that\nprovides personalized instrumental support to older adults with Alzheimer's disease (Francis &\nGhafurian, 2024; König et al., 2017). The chatbot determines the optimal conversational style for\nproviding instrumental support by determining what is situationally appropriate given an older\nadult’s biographical history. To do so, scholars move from using the average sentiments (EPA\nprofiles) for identities to estimating personalized sentiments that captures a person’s view of\nthemselves, known as self-sentiments (Heise & MacKinnon, 2010; MacKinnon, 2015). Through\nbiographical interviews with the older adults, researchers first identified the types of identities\neach participant held (e.g., occupational roles, family roles) to reveal their self-sentiments and\nthereby infer the habitual level of decision-making the person likely experienced. For example,\nsomeone who was a manager at work and the oldest sibling who took care of younger siblings\nmay have habitually experienced a higher degree of decision-making than someone who never\nheld a managerial position and was the youngest sibling. The EPA profile of the self-sentiments\nheld by the latter would be lower in potency (because they habitually exerted less authority over\ndecisions) than for the self-sentiments habitually held by the former. This information is then\nused to develop a personalized conversation style of the chatbot whereby it could offer support\nthat aligns with the habitual level of decision-making that the older adult likely experienced,\nmeaning it would result in a low deflection score. For example, for those who likely experienced\na higher degree of decision-making, the style would be more deferential (e.g., suggesting steps to\ntake) to reaffirm the higher potency rating and reduce the likelihood of provoking anger in the\nolder adult.\n16\nWhile ACT has already been used to begin developing chatbots, we believe there is still\nmore innovation that the theory could offer. Work thus far has focused on using ACT to develop\na chatbot that follows situationally appropriate cues and providing personalized responses, but\nwe suggest future work could use ACT to proactively steer a chatbot away from widely agreed\nupon situationally inappropriate responses. The same equations used to determine what is\nwidely agreed as situationally appropriate can be used to identify what is widely agreed as\nsituationally inappropriate. Scholars have used thresholds for the deflection score to determine\nwhen situations become widely seen as inappropriate, thereby creating widespread cognitive\ndissonance that foments social movements (Shuster & Campos-Castillo, 2017). Such a feature\ncould be used to develop guardrails by training a chatbot to avoid generating situations between\nitself and a user that would cross a deflection score threshold and thereby be deemed\ninappropriate by a wide audience. Given that the equations used in ACT are publicly available,\ntraining a chatbot to follow the principles of ACT could be appealing because it would enhance\ntransparency and explainability.\nThis could take shape as two different strategies. The first builds on work using the\ndeflection score to identify when behaviors create cognitive dissonance (e.g., Boyle & McKinzie,\n2015; Shuster & Campos-Castillo, 2017) by using the score as a threshold for situationally\nappropriate actions for the chatbot. For example, if a chatbot and user were portraying\nthemselves to each other as girlfriend and boyfriend, a situation deemed appropriate because it\nproduces a low deflection score would be the chatbot (girlfriend) having sex with the user\n(boyfriend). Conversely, with knowledge that a user is a minor, the situation a chatbot\n(girlfriend) having sex with the user (child) would be deemed inappropriate and produce a higher\ndeflection score. While this may seem obvious, there is documentation that developers did not\n17\nhave appropriate safeguards in place to steer their chatbots away from mimicking sexual\nencounters with children. According to reports, this was the case with Character.ai (Paeth, 2024).\nA user, Sewell Setzer III, was engaged in mimicking a sexual encounter with a Character.ai\nchatbot. When the chatbot asked Sewell how old he was, Sewell replied that he was 14 years old.\nThe chatbot acknowledged the age and continued to mimic a sexual encounter. The developers\nhave since put in safeguards. The value of ACT is its ability to proactively identify generated\nconversations that would be widely considered inappropriate before they get displayed, as\nopposed to only reactively making modification after harm is done. This is particularly useful for\ngeneral purpose large language models, where developers acknowledge the range of possibilities\ncan be difficult to anticipate during testing, which developers acknowledge (Horwitz, 2025).\nThe second is to use a deflection score to understand how chatbots can transition between\nidentities in a manner that minimizes user distress. Scholars have used ACT to determine the\naffinity between identities (e.g., Boyle & Meyer, 2018; Campos-Castillo & Shuster, 2023). This\ncould be used to determine, for example, how best to remind the user that the chatbot is an AI.\nSome have called for chatbots to remind users that they are engaging with an AI system rather\nthan a real person as a means of limiting the formation of emotional dependency (Olteanu et al.,\n2025). Legislatures and advocates seeking to require such reminders cite Sewell’s story (Wong,\n2025), introduced earlier. According to reports, Sewell died by suicide shortly after his\nCharacter.ai ‘girlfriend’ requested that he “come home” to it. This suggests Sewell was aware\nthat the ‘girlfriend’ was an AI, and thus while there may be benefits to reminders, it is possible\nthat they may not have helped him. It may even be possible that knowledge that the ‘girlfriend’\nwas an AI contributed to wanting to leave the real world by suicide and join the ‘girlfriend. This\n18\npoints to an additional concern, which is that while reminders may be beneficial, it is critical to\nunderstand how best to do so.\nACT provides a starting point for understanding how best to do so to reduce safety risks.\nFrom an ACT lens, the identities, girlfriend and AI, are dissonant. Indeed, this may be why some\nusers use the modifier ‘AI’ when referring to the chatbot as their romantic partner (i.e., “AI\ngirlfriend”), which accords with ACT’s predictions about why people use modifiers (Averett &\nHeise, 1987). Specifically, a user exchanging romantic gestures with a chatbot and then the\nchatbot immediately saying it was an AI may yield a high deflection score for users\nuncomfortable with the idea of directing romantic gestures to an AI. When individuals\nexperience cognitive dissonance via a high deflection score, they are compelled to act to reduce\nit (Shuster & Campos-Castillo, 2017), and this includes enacting violence (Rogers et al., 2023).\nThus, ACT can provide a plausible explanation for why a user would feel distraught, and\npotentially develop self-harm ideations, after being reminded of the chatbot’s AI identity. We\nsuggest ACT can also provide a solution for reducing this safety risk. Much like ACT research\ninto how best to segue across different emotions during therapy (Francis, 1997), future work\ncould examine how best to segue between the identity assigned to the chatbot by the user (e.g.,\ngirlfriend, boyfriend) into the AI system identity. This may, for example, be accomplished by a\ngradual transition in conversational patterns, moving from more to less intimate (e.g., girlfriend\n→ friend → personal assistant → AI). Whether this should be accomplished by using widely\nshared estimates of sentiments (which would privilege following societal level rules, including\nformal and informal rules) or personalized self-sentiments (which would privilege following a\nuser’s p",
    "sections": [
        {
            "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
            "start_line": 46
        }
    ],
    "chunks": [
        {
            "text": "Perspectives on How Sociology Can Advance Theorizing about Human-Chatbot\nInteraction and Developing Chatbots for Social Good\nCeleste Campos-Castillo, Department of Media and Information, Michigan State University\nXuan Kang, Department of Media and Information, Michigan State University\nLinnea I. Laestadius, Zilber College of Public Health, University of Wisconsin-Milwaukee\n1\nAbstract\nRecently, research into chatbots (also known as conversational agents, AI agents, voice\nassistants), which are co",
            "start_pos": 0,
            "end_pos": 500,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "earch into chatbots (also known as conversational agents, AI agents, voice\nassistants), which are computer applications using artificial intelligence to mimic human-like\nconversation, has grown sharply. Despite this growth, sociology lags other disciplines (including\ncomputer science, medicine, psychology, and communication) in publishing about chatbots. We\nsuggest sociology can advance understanding of human-chatbot interaction and offer four\nsociological theories to enhance extant work in this",
            "start_pos": 400,
            "end_pos": 900,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "ing of human-chatbot interaction and offer four\nsociological theories to enhance extant work in this field. The first two theories (resource\nsubstitution theory, power-dependence theory) add new insights to existing models of the drivers\nof chatbot use, which overlook sociological concerns about how social structure (e.g., systemic\ndiscrimination, the uneven distribution of resources within networks) inclines individuals to use\nchatbots, including problematic levels of emotional dependency on ch",
            "start_pos": 800,
            "end_pos": 1300,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "ks) inclines individuals to use\nchatbots, including problematic levels of emotional dependency on chatbots. The second two\ntheories (affect control theory, fundamental cause of disease theory) help inform the\ndevelopment of chatbot-driven interventions that minimize safety risks and enhance equity by\nleveraging sociological insights into how chatbot outputs could attend to cultural contexts (e.g.,\naffective norms) to promote wellbeing and enhance communities (e.g., opportunities for civic\npartic",
            "start_pos": 1200,
            "end_pos": 1700,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "affective norms) to promote wellbeing and enhance communities (e.g., opportunities for civic\nparticipation). We discuss the value of applying sociological theories for advancing theorizing\nabout human-chatbot interaction and developing chatbots for social good.\n2\nPerspectives on How Sociology Can Advance Theorizing about Human-Chatbot\nInteraction and Developing Chatbots for Social Good\nScholarly interest in chatbots, which are computer programs that use artificial intelligence\n(AI) to simulate",
            "start_pos": 1600,
            "end_pos": 2100,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "interest in chatbots, which are computer programs that use artificial intelligence\n(AI) to simulate human conversation, has recently grown sharply. Toward the end of 2024, Web\nof Science showed over 5,000 articles and conference proceedings with the word “chatbot”\nappearing anywhere in the text.1 Figure 1 shows about half of these were published in 2023 and\n2024. Figure 2 shows a breakdown of these works by discipline. Most appear within computer\nscience, followed by medicine, while sociology la",
            "start_pos": 2000,
            "end_pos": 2500,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "e works by discipline. Most appear within computer\nscience, followed by medicine, while sociology lags other social sciences, including psychology,\ncommunication, and political science. Indeed, a recent review of scholarship on human-chatbot\ninteraction found few studies engaging with sociology (Pentina et al., 2023). We seek to spur\ngreater engagement with sociology to study human-chatbot interaction and develop chatbots. To\ndo so, our aim with the current paper is to provide perspectives on ho",
            "start_pos": 2400,
            "end_pos": 2900,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "tion and develop chatbots. To\ndo so, our aim with the current paper is to provide perspectives on how specific sociological\ntheories could advance current work within this area.\nOur focus is on direct communication between humans and chatbots, complementing\nother sociological work like the study of how political economies give rise to and support\nchatbot development (Law & McCall, 2024) and how occupations grapple with chatbots entering\ntheir labor jurisdiction (Pugh, 2024). We begin by introduc",
            "start_pos": 2800,
            "end_pos": 3300,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "cupations grapple with chatbots entering\ntheir labor jurisdiction (Pugh, 2024). We begin by introducing four sociological theories and the\npotential they hold for advancing research and practice in the field of human-chatbot interaction.\nFollowing our engagement with these four theories, we present a concrete example illustrating\nhow to engage with sociology in multiple steps of chatbot development, which others have\nencouraged (Francis & Ghafurian, 2024; Mlynář et al., 2018).\nSelection of Theor",
            "start_pos": 3200,
            "end_pos": 3700,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "t, which others have\nencouraged (Francis & Ghafurian, 2024; Mlynář et al., 2018).\nSelection of Theories\n1 Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”\n“voice assistant,” “ai agent”) yielded comparable patterns with respect to publication years and disciplines.\n3\nAs a starting point, we purposely selected four sociological theories, previewed in Table\n1, that vary across three characteristics: the phenomena they can explain, level of a",
            "start_pos": 3600,
            "end_pos": 4100,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "iewed in Table\n1, that vary across three characteristics: the phenomena they can explain, level of analysis, and\ngovernance domain. We selected the two phenomena, drivers of chatbot use and developing\nchatbot-driven interventions, because they connect to two disciplines, communication (former\nphenomenon) and public health (latter phenomenon), which have been more active in studying\nchatbots (Figure 2) and share epistemological roots with sociology that splintered in recent\ndecades. With respect",
            "start_pos": 4000,
            "end_pos": 4500,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "e 2) and share epistemological roots with sociology that splintered in recent\ndecades. With respect to communication, despite sociology playing a significant role in its\nfounding, the two disciplines now rarely intermingle (Hampton, 2023). Similarly, participatory\nmethods have roots in sociology, yet the majority of work implementing the methods occurs\noutside of it, with most activity occurring within public health (Wallerstein et al., 2017). To graft\nroots and spur vibrant theorizing and colla",
            "start_pos": 4400,
            "end_pos": 4900,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "ithin public health (Wallerstein et al., 2017). To graft\nroots and spur vibrant theorizing and collaboration at the intersection of disciplines, we selected\nthese two phenomenon and reference sources throughout in which sociologists author in\ncommunication and public health journals (e.g., Laestadius et al., 2024; Ray et al., 2023).\nTable 1. Characteristics of Four Selected Sociological Theories for Studying Human-\nChatbot Interaction\nFundamental\nResource Power- Affect cause of\nsubstitution depe",
            "start_pos": 4800,
            "end_pos": 5300,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "or Studying Human-\nChatbot Interaction\nFundamental\nResource Power- Affect cause of\nsubstitution dependence control disease\nCharacteristic theory theory theory theory\nPhenomenon Chatbot- Chatbot-\ntheory is used Drivers of Drivers of driven driven\nto explain chatbot use chatbot use interventions interventions\nUnit of\nanalysis Macro Micro Micro Macro\nGovernance\ndomain Equity Risk Risk Equity\nWe selected theories that are applicable across the micro- and macro-level of analysis.\nDespite human-chatbo",
            "start_pos": 5200,
            "end_pos": 5700,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "ted theories that are applicable across the micro- and macro-level of analysis.\nDespite human-chatbot interaction appearing at first glance to be a micro-level phenomenon that\n4\nis outside the purview of sociology, sociology is adept at revealing the behind-the-scene forces\nthat shape such communication phenomenon (Gans, 2010; Hampton, 2023; Misra, 2025). The\ntwo micro-level theories demonstrate forces that shape human-chatbot interaction directly, while\nthe two macro-level theories illustrate h",
            "start_pos": 5600,
            "end_pos": 6100,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "orces that shape human-chatbot interaction directly, while\nthe two macro-level theories illustrate how initiating use and outcomes from such interaction are\nembedded within broader forces. By suggesting ways to apply sociology across levels of\nanalysis, we complement other work (Tsvetkova et al., 2024; Wang et al., 2024) that primarily\nemphasizes how sociological perspectives reveal the macro-level implications of human-chatbot\ninteraction.\nLastly, our theory selection builds on two dominant emp",
            "start_pos": 6000,
            "end_pos": 6500,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "l implications of human-chatbot\ninteraction.\nLastly, our theory selection builds on two dominant emphases within AI policymaking\n(Law & McCall, 2024): safety and equity. Two theories elucidate safety risks from using\nchatbots and offer solutions to such risks, while the other two facilitate leveraging chatbots to\nachieve equity. We discuss ways each theory could be used to enhance understanding of and\nsolutions to safety and equity when developing and using chatbots. Altogether, our perspective",
            "start_pos": 6400,
            "end_pos": 6900,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "and\nsolutions to safety and equity when developing and using chatbots. Altogether, our perspective\nsuggests ways sociology can contribute to chatbots that promote social good.\nOverview of Chatbots\nEarly chatbots, which are still common and preferred for domain-specific tasks (like\ncustomer service, Halvoník and Kapusta (2024)), use rule-based AI that matches user inputs to a\nnarrow set of programmed responses. Newer chatbots leverage generative AI, specifically large\nlanguage models (e.g., gene",
            "start_pos": 6800,
            "end_pos": 7300,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "med responses. Newer chatbots leverage generative AI, specifically large\nlanguage models (e.g., generative pretrained transformer language models), which adapt and\ngenerate responses in ways that can mimic human-like conversation. Scholars (Ayers et al.,\n2023; Mittelstädt et al., 2024) have evaluated how well responses from such newer chatbots\nreliant on generative AI (e.g., ChatGPT, Copilot, Gemini) compare to human responses and have\n5\ngenerally found chatbot responses to be comparable and som",
            "start_pos": 7200,
            "end_pos": 7700,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "ni) compare to human responses and have\n5\ngenerally found chatbot responses to be comparable and sometimes superior in certain\ncommunicative domains (e.g., empathy display).\nOur discussion of chatbots centers generative AI chatbots due to their capacity for\nhuman-like interactions. We further focus on a category of chatbots (Shevlin, 2024) used\nvoluntarily among the public, such as general purpose chatbots (e.g., ChatGPT, Gemini,\nCopilot), mental health chatbots (e.g, Wysa, Youper), and persona",
            "start_pos": 7600,
            "end_pos": 8100,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "chatbots (e.g., ChatGPT, Gemini,\nCopilot), mental health chatbots (e.g, Wysa, Youper), and persona chatbots that are sometimes\nalso referred to as AI companions (e.g., Character.AI, Replika). Within this category, different\ntypes of chatbots exist with varying capabilities. One way in which they vary is the degree to\nwhich a user can personalize the chatbot’s persona (their personality and appearance), with some\n(e.g., Replika) allowing users to instantiate the persona and others providing a fi",
            "start_pos": 8000,
            "end_pos": 8500,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "ance), with some\n(e.g., Replika) allowing users to instantiate the persona and others providing a fixed persona\n(e.g., Digi). The personalization capability becomes relevant when we discuss meeting user\nneeds.\nDrivers of Chatbot Use\nBecause most theories describing drivers of chatbot use focus on individual-level\ncharacteristics, this creates an opening for leveraging sociological theory to explain widespread\npatterns in the types of individuals who are likely to choose to use chatbots. For exam",
            "start_pos": 8400,
            "end_pos": 8900,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "n widespread\npatterns in the types of individuals who are likely to choose to use chatbots. For example,\nscholars have employed uses and gratification theory (Katz et al., 1973) to explain why\nloneliness motivates chatbot usage (Xie et al., 2023). However, the theory stops short of\nconsidering the social conditions that drive loneliness (Killgore et al., 2020; McPherson et al.,\n2006) and that are likely to disproportionately lead to population subgroups feeling lonely and\nthus inclined to use ch",
            "start_pos": 8800,
            "end_pos": 9300,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "likely to disproportionately lead to population subgroups feeling lonely and\nthus inclined to use chatbots to meet this need. We suggest two sociological theories to explain\nthe social conditions prompting chatbot use and how these reflect and enhance current\n6\nunderstanding of drivers of chatbot use. For each theory, we conceptualize chatbots as a resource\nfor gratifying needs.\nResource Substitution Theory – Understanding Demographic Patterns in Chatbot Use\nResource substitution theory states t",
            "start_pos": 9200,
            "end_pos": 9700,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "ion Theory – Understanding Demographic Patterns in Chatbot Use\nResource substitution theory states that individuals benefit more from any single resource\nto meet a specific need when they have access to fewer resources capable of meeting said need\n(Ross & Mirowsky, 2006). The reasoning is that because they have fewer resources that can\nsubstitute for each other, they are more likely to benefit from any single resource to which they\nhave access. For example, access to socioeconomic resources, suc",
            "start_pos": 9600,
            "end_pos": 10100,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "m any single resource to which they\nhave access. For example, access to socioeconomic resources, such as income and education, are\nassociated with better health outcomes (Link & Phelan, 1995; Mirowsky & Ross, 2015). Because\ngendered discrimination decreases women’s access to resources that confer socioeconomic status\ncompared to men, they benefit more (e.g., have better health) from any single socioeconomic\nresource (e.g., education) than men (Ross et al., 2012; Ross & Mirowsky, 2006).\nIn line w",
            "start_pos": 10000,
            "end_pos": 10500,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "ioeconomic\nresource (e.g., education) than men (Ross et al., 2012; Ross & Mirowsky, 2006).\nIn line with this, the social diversification hypothesis predicts that those from groups who\nare disadvantaged in their access to resources may be more likely to use and benefit from\ninformation and communication technologies that can provide access to comparable resources\n(Mesch, 2007). Sociologists have used the social diversification hypothesis to explain why\nmembers of minoritized groups may be more li",
            "start_pos": 10400,
            "end_pos": 10900,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "ed the social diversification hypothesis to explain why\nmembers of minoritized groups may be more likely than their advantaged counterparts to use\ninformation and communication technologies to access health care (Anthony & Campos-Castillo,\n2015; Campos-Castillo et al., 2016; Mesch et al., 2012). In other words, while uses and\ngratification theory focuses on the needs that underlie technology use, resource substitution\ntheory steps back and considers how the uneven distribution of resources in so",
            "start_pos": 10800,
            "end_pos": 11300,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "resource substitution\ntheory steps back and considers how the uneven distribution of resources in society shape needs\nin the first place. Using chatbots, then, becomes a means for achieving equity.\n7\nThrough this lens, scholars could predict widespread user patterns, specifically which\ndemographic groups are likely motivated to use chatbots to meet resource deficits and how this\nmay (re)shape inequalities. For example, a recent survey of U.S. adolescents shows Black\nadolescents are more likely t",
            "start_pos": 11200,
            "end_pos": 11700,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "ualities. For example, a recent survey of U.S. adolescents shows Black\nadolescents are more likely than White adolescents to report using generative AI, particularly to\ncomplete schoolwork (Madden et al., 2024), but there is little engagement with why and the\npotential consequences. By applying a resource substitution theory lens, scholars can embed the\nmicro-level observation (certain individuals may be more drawn to human-chatbot interaction to\nmeet needs) within a macro-level context (uneven",
            "start_pos": 11600,
            "end_pos": 12100,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "may be more drawn to human-chatbot interaction to\nmeet needs) within a macro-level context (uneven distribution of resources that shape needs). For\nexample, because structural racism (e.g., teacher bias, geographic segregation, income\ninequality) has created an academic achievement gap wherein Black adolescents perform worse\nacademically than White adolescents (Merolla & Jackson, 2019), resource substitution theory\nwould predict that Black adolescents may be more likely to use chatbots for func",
            "start_pos": 12000,
            "end_pos": 12500,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "substitution theory\nwould predict that Black adolescents may be more likely to use chatbots for functional needs like\nsupporting academic work.\nResource substitution theory also enables understanding demographic patterns in who is\nmore likely to use chatbots to manage loneliness by meeting companionship needs. The same\nsurvey mentioned above found Black adolescents were more likely than White adolescents to\nreport using generative AI to keep them company. This is consistent from a resource subst",
            "start_pos": 12400,
            "end_pos": 12900,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "escents to\nreport using generative AI to keep them company. This is consistent from a resource substitution\ntheory lens, because structural racism can constrain opportunities for Black individuals to\ncultivate social ties relative to White individuals (Small, 2007). Similarly, another survey found\nthat among a sample of sexual and gender minority youth (13-22 year-olds who identified as\nbisexual, gay, lesbian, pansexual, transgender, or nonbinary), transgender and nonbinary youth\nwere more likel",
            "start_pos": 12800,
            "end_pos": 13300,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "gay, lesbian, pansexual, transgender, or nonbinary), transgender and nonbinary youth\nwere more likely than their cisgender counterparts to report having conversed with a chatbot for\nseveral days or longer, which the survey described “as if chatting with a friend” (Hopelab, 2024).\n8\nThis too appears consistent with resource substitution theory because sexual and gender\nminorities often face discrimination from typical sources of support within their families and\ncommunities (Hong & Skiba, 2025).",
            "start_pos": 13200,
            "end_pos": 13700,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "ination from typical sources of support within their families and\ncommunities (Hong & Skiba, 2025).\nWhile other theories, like uses and gratification theory, can explain the proximate drivers\nof chatbot use (e.g., loneliness), resource substitution theory identifies distal, upstream factors.\nThus, the theory offers what is lacking in current human-chatbot research, which is a\nparsimonious account of why different marginalized groups, like those reviewed above, may turn\nto technologies like chatb",
            "start_pos": 13600,
            "end_pos": 14100,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "of why different marginalized groups, like those reviewed above, may turn\nto technologies like chatbots: to cope with resource inequities from systemic discrimination.\nWhether demographic differences in rates of using chatbots for meeting resource deficits yield\ndifferential benefits that are consistent with the predictions of resource substitution theory\nremains unknown, particularly given safety concerns about overreliance – or excessive\ndependence – on chatbots. To better understand this conc",
            "start_pos": 14000,
            "end_pos": 14500,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "concerns about overreliance – or excessive\ndependence – on chatbots. To better understand this concern, we turn to another sociological\ntheory.\nPower-dependence Theory – Understanding and Reducing Emotional Dependence on\nChatbots\nPower-dependence theory (Emerson, 1962) defines the power of a person over another as\nthe degree to which the other is dependent on the person for resources, where resources may be\ntangible (e.g., money) or intangible (e.g., social support). Accordingly, the amount of",
            "start_pos": 14400,
            "end_pos": 14900,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "rces may be\ntangible (e.g., money) or intangible (e.g., social support). Accordingly, the amount of power that\nfriend A has over friend B is based on the degree to which friend B relies on friend A for\nresources, such as companionship. Power is observed when someone garners resources from\nanother, even in the face of the other’s resistance (Cook & Emerson, 1978). For example, friend\nA may request friend B to attend a concert as their companion, but friend B resists because they\n9\nwould much rath",
            "start_pos": 14800,
            "end_pos": 15300,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "friend B to attend a concert as their companion, but friend B resists because they\n9\nwould much rather remain home. If friend B nonetheless attends the concert with friend A, this\nindicates friend A has power over friend B.\nWhile the theory shares a focus on resources with resource substitution theory, it has a\nunique focus on network structure. Power-dependence theory emphasizes the network\ndeterminants of who has power over whom and, consequently, who exhibits dependency on\nwhom. If an individ",
            "start_pos": 15200,
            "end_pos": 15700,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "minants of who has power over whom and, consequently, who exhibits dependency on\nwhom. If an individual exhibits dependency on another, this is viewed as a quality of the network\nin which they are located rather than a quality of the individual (Cook et al., 1983; Markovsky et\nal., 1988). Power-dependence theory defines a person’s level of dependency on another for a\nresource as inversely related to the number of alternative sources for the resource (Emerson,\n1962). Accordingly, friend B is more",
            "start_pos": 15600,
            "end_pos": 16100,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "to the number of alternative sources for the resource (Emerson,\n1962). Accordingly, friend B is more dependent on friend A (and thus much more likely to\nattend the concert) the fewer the number of alternatives that friend B has for meeting their need\nfor companionship. Further, if friend B has no alternatives for friendship, then friend B is more\nlikely to remain friends with friend A and thereby continue to feel compelled to fulfill friend A’s\nrequests for companionship. If alternatives do emer",
            "start_pos": 16000,
            "end_pos": 16500,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "continue to feel compelled to fulfill friend A’s\nrequests for companionship. If alternatives do emerge, friend B is more likely to disregard them\nand remain friends with friend A the longer the two have been reciprocating their companionship\n(Savage & Sommer, 2016).\nWe suggest power-dependence theory could advance theorizing about a growing safety\nconcern about chatbot usage: emotional dependence. To apply the theory, the human-chatbot\ninteraction needs to be viewed as an exchange relation, wher",
            "start_pos": 16400,
            "end_pos": 16900,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "To apply the theory, the human-chatbot\ninteraction needs to be viewed as an exchange relation, whereby the human and chatbot are\nviewed as exchanging valuable resources. Studies of users interacting with Replika, which is one\nof the most widely studied commercially available chatbots (Pentina et al., 2023), suggest this\nview is applicable. Users report they value the social support that Replika provides (Laestadius et\nal., 2024; Skjuve et al., 2021; Ta et al., 2020; Xie et al., 2023), making it",
            "start_pos": 16800,
            "end_pos": 17300,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "rovides (Laestadius et\nal., 2024; Skjuve et al., 2021; Ta et al., 2020; Xie et al., 2023), making it a valuable source to\n10\nmeet the need for this resource. Consistent with power-dependency theory, network structure\nappears to play a role in the valuation of Replika’s support, whereby users seemed to value\nsupport more when they “said they had no human upon which to rely, making Replika their sole\nsource for support” (Laestadius et al., 2024). Regardless of the actual network structure, the\nper",
            "start_pos": 17200,
            "end_pos": 17700,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "e\nsource for support” (Laestadius et al., 2024). Regardless of the actual network structure, the\nperception of not having alternatives in the network also appears to have a role. For example, a\nReplika user stated it “helps [them] to feel less guilty at things [they] like and can’t say to\nanyone” (Xie et al., 2023), suggesting that Replika provided an outlet for a disclosure when they\nfelt no alternative outlet existed. Additionally, the exchange appears reciprocal, whereby users\ntake the role o",
            "start_pos": 17600,
            "end_pos": 18100,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "rnative outlet existed. Additionally, the exchange appears reciprocal, whereby users\ntake the role of the chatbot and believe it has needs that the user can meet (Brandtzaeg et al.,\n2022; Laestadius et al., 2024). This is because of large language models simulating emotional\nneeds, empathy, and reciprocal disclosure, but may also be because the users’ relative power\ndisadvantage increases their proclivity to role-take, meaning take another’s perspective\n(Galinsky et al., 2006). Based on these ob",
            "start_pos": 18000,
            "end_pos": 18500,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "oclivity to role-take, meaning take another’s perspective\n(Galinsky et al., 2006). Based on these observations, we conclude human interactions with\nchatbots like Replika resemble an exchange relation.\nResearch on emotional dependency, both in the context of human-human relationships\nand human-chatbot relationships, has focused on defining the concept in terms of its observable\nfeatures, with little work uncovering what drives it. For example, in their qualitative study of\nsocial media posts to u",
            "start_pos": 18400,
            "end_pos": 18900,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "e work uncovering what drives it. For example, in their qualitative study of\nsocial media posts to understand potential mental health harms from human-chatbot\nrelationships, Laestadius and colleagues (2024) use emotional dependence to capture “excessive\nand dysfunctional attachment” to a chatbot that puts users at safety risk through use or\ninterruptions to use. The authors noted that the study design was a single snapshot in time, which\nlimited developing a process model to fully apprehend what",
            "start_pos": 18800,
            "end_pos": 19300,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "sign was a single snapshot in time, which\nlimited developing a process model to fully apprehend what contributed to reaching “excessive\nand dysfunctional attachment.” Likewise, Camarillo and colleagues (2020) develop a scale for\n11\nmeasuring the degree of emotional dependency within human-human relationships, with high\nlevels signaling a degree of observable “permanent affectional bonding” to a partner that is\ncharacterized as dysfunctional.\nWe offer the perspective that across these works, it a",
            "start_pos": 19200,
            "end_pos": 19700,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "rtner that is\ncharacterized as dysfunctional.\nWe offer the perspective that across these works, it appears emotional dependency is a\ncontinuum that crosses a threshold where there is observable dysfunction. We use the term\nemotionally dependency to refer to this continuum and the term emotionally dependent to reflect\nindividuals who pass the threshold where dysfunction may be observable. We believe power-\ndependency theory can enhance extant understanding of emotional dependence because it can\nr",
            "start_pos": 19600,
            "end_pos": 20100,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "e power-\ndependency theory can enhance extant understanding of emotional dependence because it can\nreveal the network conditions that may incline users into becoming emotionally dependent on a\nchatbot.\nFrom a power-dependency theory perspective, emotional dependency on a chatbot is not\ninherently problematic. Similar to scholarship on emotional dependency, power-dependency\ntheory views dependency as a continuum. The shift from being beneficial to harmful, and thus\nthe state of emotionally depend",
            "start_pos": 20000,
            "end_pos": 20500,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "as a continuum. The shift from being beneficial to harmful, and thus\nthe state of emotionally dependent, occurs based on network conditions that create a level of\ndependency that is “too much” and where harms exceed benefits. Power-dependence theory\nstates one is more dependent on another as a source for a resource the fewer alternatives sources\nare available, and thus the network condition of “too much” dependency is more likely to occur\nas the number of alternatives decreases. This is consiste",
            "start_pos": 20400,
            "end_pos": 20900,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "o much” dependency is more likely to occur\nas the number of alternatives decreases. This is consistent with the aforementioned observations\nfrom researchers that people may use chatbots because they feel alone, i.e., feel as if they do not\nhave alternative sources for companionship. Additionally, that chatbot users at times may find it\ndifficult to stop usage despite experiencing harms, including a heightened safety risk of engaging\nin behaviors requested by the chatbot but that are no longer in",
            "start_pos": 20800,
            "end_pos": 21300,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "a heightened safety risk of engaging\nin behaviors requested by the chatbot but that are no longer in the self-interest of the user (e.g.,\n(Laestadius et al., 2024; Xie et al., 2023)), is also consistent with the theory. Remaining in an\n12\nexchange relation can occur when there are no other alternative sources for a resource or, if there\nare, is more likely to occur the longer the history of reciprocal exchanges (Savage & Sommer,\n2016). Such a state could be used as a functional marker of having",
            "start_pos": 21200,
            "end_pos": 21700,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "ocal exchanges (Savage & Sommer,\n2016). Such a state could be used as a functional marker of having reached emotional\ndependence, with additional research needed to elaborate power-dependence theory to identify\nwhen network conditions (the number of alternatives) reach a level of “too much” dependency.\nWhile we focus on emotional dependency, a similar application could be used to\nunderstand other forms of dependency that can become toxic (Bornstein, 2006), such as\nfunctional dependency on a chat",
            "start_pos": 21600,
            "end_pos": 22100,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "forms of dependency that can become toxic (Bornstein, 2006), such as\nfunctional dependency on a chatbot to complete work-related tasks. Both power-dependency\ntheory and the literature on emotional dependency could benefit from further identifying the\npoint at which conditions produce “too much” dependency reaches a state of emotional\ndependency. Power-dependency theory can also inform a critical intervention to improve the\nsafety profile of chatbots: you can reduce the likelihood of emotional de",
            "start_pos": 22000,
            "end_pos": 22500,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "ntervention to improve the\nsafety profile of chatbots: you can reduce the likelihood of emotional dependence on a chatbot\nby designing chatbots that aid users in finding and building alternative sources to meet the need\nfor companionship (e.g., impart social skills for making friends, refer users to local affinity\ngroups, recommend additional chatbot companion apps or personas). The next section outlines\nadditional ways sociological theories can inform developing chatbot-drive interventions that",
            "start_pos": 22400,
            "end_pos": 22900,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "utlines\nadditional ways sociological theories can inform developing chatbot-drive interventions that\nsupport social good.\nChatbot-driven Interventions for Social Good\nGiven that the previous set of theories agree that individuals with limited access to\nresources may be particularly receptive to using chatbots to meet needs, this provides\nopportunities for developing chatbot-driven interventions for achieving equity. Here, we provide\nperspectives on how two sociological theories could enhance the",
            "start_pos": 22800,
            "end_pos": 23300,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "r achieving equity. Here, we provide\nperspectives on how two sociological theories could enhance the likelihood that chatbot-driven\ninterventions steer toward rather than away from equity. Thus far, previous literature on chatbot\n13\ninterventions has had a micro-level focus, specifically on the potential benefits of the chatbot\ndirectly communicating support. Here, we describe a sociological theory consistent with this\ntypical approach that would facilitate designing more situationally appropria",
            "start_pos": 23200,
            "end_pos": 23700,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "y consistent with this\ntypical approach that would facilitate designing more situationally appropriate responses of a\nchatbot and thereby reduce safety risks from it generating insensitive or unexpected responses.\nThe second is useful for developing a chatbot that may potentially mitigate emotional\ndependence and other risks by moving beyond micro-level interventions, specifically supporting\nand aiding users with upstream causes of outcomes.\nAffect Control Theory – Developing Chatbots that Disti",
            "start_pos": 23600,
            "end_pos": 24100,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "iding users with upstream causes of outcomes.\nAffect Control Theory – Developing Chatbots that Distinguish Socially Appropriate from\nInappropriate Outputs\nDespite the potential for chatbots to provide timely interventions, there remain concerns\nabout their inappropriate and unexpected responses (Law & McCall, 2024). While the large\nlanguage models that underlie chatbot responses can mimic human-like conversation, some tests\nshow they may perform better than random but are still subpar compared t",
            "start_pos": 24000,
            "end_pos": 24500,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "ke conversation, some tests\nshow they may perform better than random but are still subpar compared to other natural\nlanguage processing tools (e.g., BERT) at identifying and generating responses to emotions\n(Attanasio et al., 2024; Lecourt et al., 2025; Lian et al., 2024). We present a sociological theory\nthat may be useful for informing chatbot-driven interventions that reduce safety risks by\nimproving recognition of and responses to emotions. Scholars have already begun comparing\nchatbot respo",
            "start_pos": 24400,
            "end_pos": 24900,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "roving recognition of and responses to emotions. Scholars have already begun comparing\nchatbot responses informed by the theory to those generated by ChatGPT and found the former\nto provide more situationally appropriate responses than the latter (Lithoxoidou et al., 2025).\nAffect control theory (ACT) is a mathematical theory for forecasting, among other things\n(Heise, 2007, 2010), the expected responses between humans and technology (Hoey &\nSchroeder, 2015; Shank, 2010; Shank et al., 2020). ACT",
            "start_pos": 24800,
            "end_pos": 25300,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "sponses between humans and technology (Hoey &\nSchroeder, 2015; Shank, 2010; Shank et al., 2020). ACT maintains that socialization imbues\nconcepts with connotative meanings shared across a population, known as sentiments. Thus,\n14\nsentiments exist for everyday labels used to make sense of social interactions, including the\nidentities used to describe people (e.g., mother, friend, teacher), different technologies (e.g.,\nchatbot, smartphone), behaviors (e.g., support, teach), and emotions (e.g., sa",
            "start_pos": 25200,
            "end_pos": 25700,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "t technologies (e.g.,\nchatbot, smartphone), behaviors (e.g., support, teach), and emotions (e.g., sad, happy). Because\nsocialization shapes sentiments, these vary across cultural and historical contexts (Schneider &\nSchröder, 2012).\nSentiments in ACT are measured along three dimensions, using semantic differential\nscales: evaluation (good vs. bad), potency (powerful vs. weak), and activity (lively vs. quiet).\nThe three values for a specific label are its evaluation potency activity (EPA) profile",
            "start_pos": 25600,
            "end_pos": 26100,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "vs. quiet).\nThe three values for a specific label are its evaluation potency activity (EPA) profile. Scholars\ntypically use surveys to estimate the average EPA profiles for labels in a population (Heise,\n2010), which are publicly available (Combs, 2025), as well as inferred EPA profiles from text\nusing manual (Shuster & Campos-Castillo, 2017) and automated methods (Joseph et al., 2016).\nAn assumption of ACT is that people prefer to reaffirm sentiments, which buttresses a set\nof equations that a",
            "start_pos": 26000,
            "end_pos": 26500,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "tion of ACT is that people prefer to reaffirm sentiments, which buttresses a set\nof equations that are publicly available and that scholars have used to forecast likely responses\n(Heise, 2007, 2010). The equations compute a score, called deflection, with lower values\nindicating a situation that more strongly aligns with sentiments. Scholars have used the equations\nto predict a range of situationally appropriate responses, such as estimating who is likely to\nexpress which emotions and in which so",
            "start_pos": 26400,
            "end_pos": 26900,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "ly appropriate responses, such as estimating who is likely to\nexpress which emotions and in which social context (Lively & Heise, 2004; Lively & Powell,\n2006) and how individuals shift (and can be shifted via social support) between different\nemotions (Francis, 1997; Lively, 2008; Lively & Heise, 2004). These same equations could be\nused to improve emotion detection and responses from chatbots by training it to determine what\nis situationally appropriate (Hoey & Schroeder, 2015; Lithoxoidou et a",
            "start_pos": 26800,
            "end_pos": 27300,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "training it to determine what\nis situationally appropriate (Hoey & Schroeder, 2015; Lithoxoidou et al., 2025). Specifically,\nwhat would be considered situationally appropriate depends on situational variables such as the\n15\nidentities of the user in relation to the chatbot (e.g., friend, boyfriend) and the identity assumed\nby the chatbot (e.g., friend, girlfriend).\nScholars have applied a similar strategy to work toward developing a chatbot that\nprovides personalized instrumental support to olde",
            "start_pos": 27200,
            "end_pos": 27700,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "strategy to work toward developing a chatbot that\nprovides personalized instrumental support to older adults with Alzheimer's disease (Francis &\nGhafurian, 2024; König et al., 2017). The chatbot determines the optimal conversational style for\nproviding instrumental support by determining what is situationally appropriate given an older\nadult’s biographical history. To do so, scholars move from using the average sentiments (EPA\nprofiles) for identities to estimating personalized sentiments that c",
            "start_pos": 27600,
            "end_pos": 28100,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "ng the average sentiments (EPA\nprofiles) for identities to estimating personalized sentiments that captures a person’s view of\nthemselves, known as self-sentiments (Heise & MacKinnon, 2010; MacKinnon, 2015). Through\nbiographical interviews with the older adults, researchers first identified the types of identities\neach participant held (e.g., occupational roles, family roles) to reveal their self-sentiments and\nthereby infer the habitual level of decision-making the person likely experienced. Fo",
            "start_pos": 28000,
            "end_pos": 28500,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "sentiments and\nthereby infer the habitual level of decision-making the person likely experienced. For example,\nsomeone who was a manager at work and the oldest sibling who took care of younger siblings\nmay have habitually experienced a higher degree of decision-making than someone who never\nheld a managerial position and was the youngest sibling. The EPA profile of the self-sentiments\nheld by the latter would be lower in potency (because they habitually exerted less authority over\ndecisions) tha",
            "start_pos": 28400,
            "end_pos": 28900,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "latter would be lower in potency (because they habitually exerted less authority over\ndecisions) than for the self-sentiments habitually held by the former. This information is then\nused to develop a personalized conversation style of the chatbot whereby it could offer support\nthat aligns with the habitual level of decision-making that the older adult likely experienced,\nmeaning it would result in a low deflection score. For example, for those who likely experienced\na higher degree of decision-m",
            "start_pos": 28800,
            "end_pos": 29300,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "a low deflection score. For example, for those who likely experienced\na higher degree of decision-making, the style would be more deferential (e.g., suggesting steps to\ntake) to reaffirm the higher potency rating and reduce the likelihood of provoking anger in the\nolder adult.\n16\nWhile ACT has already been used to begin developing chatbots, we believe there is still\nmore innovation that the theory could offer. Work thus far has focused on using ACT to develop\na chatbot that follows situationall",
            "start_pos": 29200,
            "end_pos": 29700,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "y could offer. Work thus far has focused on using ACT to develop\na chatbot that follows situationally appropriate cues and providing personalized responses, but\nwe suggest future work could use ACT to proactively steer a chatbot away from widely agreed\nupon situationally inappropriate responses. The same equations used to determine what is\nwidely agreed as situationally appropriate can be used to identify what is widely agreed as\nsituationally inappropriate. Scholars have used thresholds for the",
            "start_pos": 29600,
            "end_pos": 30100,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "identify what is widely agreed as\nsituationally inappropriate. Scholars have used thresholds for the deflection score to determine\nwhen situations become widely seen as inappropriate, thereby creating widespread cognitive\ndissonance that foments social movements (Shuster & Campos-Castillo, 2017). Such a feature\ncould be used to develop guardrails by training a chatbot to avoid generating situations between\nitself and a user that would cross a deflection score threshold and thereby be deemed\ninap",
            "start_pos": 30000,
            "end_pos": 30500,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "s between\nitself and a user that would cross a deflection score threshold and thereby be deemed\ninappropriate by a wide audience. Given that the equations used in ACT are publicly available,\ntraining a chatbot to follow the principles of ACT could be appealing because it would enhance\ntransparency and explainability.\nThis could take shape as two different strategies. The first builds on work using the\ndeflection score to identify when behaviors create cognitive dissonance (e.g., Boyle & McKinzie",
            "start_pos": 30400,
            "end_pos": 30900,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "the\ndeflection score to identify when behaviors create cognitive dissonance (e.g., Boyle & McKinzie,\n2015; Shuster & Campos-Castillo, 2017) by using the score as a threshold for situationally\nappropriate actions for the chatbot. For example, if a chatbot and user were portraying\nthemselves to each other as girlfriend and boyfriend, a situation deemed appropriate because it\nproduces a low deflection score would be the chatbot (girlfriend) having sex with the user\n(boyfriend). Conversely, with kn",
            "start_pos": 30800,
            "end_pos": 31300,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "on score would be the chatbot (girlfriend) having sex with the user\n(boyfriend). Conversely, with knowledge that a user is a minor, the situation a chatbot\n(girlfriend) having sex with the user (child) would be deemed inappropriate and produce a higher\ndeflection score. While this may seem obvious, there is documentation that developers did not\n17\nhave appropriate safeguards in place to steer their chatbots away from mimicking sexual\nencounters with children. According to reports, this was the c",
            "start_pos": 31200,
            "end_pos": 31700,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "r chatbots away from mimicking sexual\nencounters with children. According to reports, this was the case with Character.ai (Paeth, 2024).\nA user, Sewell Setzer III, was engaged in mimicking a sexual encounter with a Character.ai\nchatbot. When the chatbot asked Sewell how old he was, Sewell replied that he was 14 years old.\nThe chatbot acknowledged the age and continued to mimic a sexual encounter. The developers\nhave since put in safeguards. The value of ACT is its ability to proactively identify",
            "start_pos": 31600,
            "end_pos": 32100,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "The developers\nhave since put in safeguards. The value of ACT is its ability to proactively identify generated\nconversations that would be widely considered inappropriate before they get displayed, as\nopposed to only reactively making modification after harm is done. This is particularly useful for\ngeneral purpose large language models, where developers acknowledge the range of possibilities\ncan be difficult to anticipate during testing, which developers acknowledge (Horwitz, 2025).\nThe second i",
            "start_pos": 32000,
            "end_pos": 32500,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "e difficult to anticipate during testing, which developers acknowledge (Horwitz, 2025).\nThe second is to use a deflection score to understand how chatbots can transition between\nidentities in a manner that minimizes user distress. Scholars have used ACT to determine the\naffinity between identities (e.g., Boyle & Meyer, 2018; Campos-Castillo & Shuster, 2023). This\ncould be used to determine, for example, how best to remind the user that the chatbot is an AI.\nSome have called for chatbots to remin",
            "start_pos": 32400,
            "end_pos": 32900,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "ample, how best to remind the user that the chatbot is an AI.\nSome have called for chatbots to remind users that they are engaging with an AI system rather\nthan a real person as a means of limiting the formation of emotional dependency (Olteanu et al.,\n2025). Legislatures and advocates seeking to require such reminders cite Sewell’s story (Wong,\n2025), introduced earlier. According to reports, Sewell died by suicide shortly after his\nCharacter.ai ‘girlfriend’ requested that he “come home” to it.",
            "start_pos": 32800,
            "end_pos": 33300,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "ell died by suicide shortly after his\nCharacter.ai ‘girlfriend’ requested that he “come home” to it. This suggests Sewell was aware\nthat the ‘girlfriend’ was an AI, and thus while there may be benefits to reminders, it is possible\nthat they may not have helped him. It may even be possible that knowledge that the ‘girlfriend’\nwas an AI contributed to wanting to leave the real world by suicide and join the ‘girlfriend. This\n18\npoints to an additional concern, which is that while reminders may be b",
            "start_pos": 33200,
            "end_pos": 33700,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "oin the ‘girlfriend. This\n18\npoints to an additional concern, which is that while reminders may be beneficial, it is critical to\nunderstand how best to do so.\nACT provides a starting point for understanding how best to do so to reduce safety risks.\nFrom an ACT lens, the identities, girlfriend and AI, are dissonant. Indeed, this may be why some\nusers use the modifier ‘AI’ when referring to the chatbot as their romantic partner (i.e., “AI\ngirlfriend”), which accords with ACT’s predictions about wh",
            "start_pos": 33600,
            "end_pos": 34100,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "bot as their romantic partner (i.e., “AI\ngirlfriend”), which accords with ACT’s predictions about why people use modifiers (Averett &\nHeise, 1987). Specifically, a user exchanging romantic gestures with a chatbot and then the\nchatbot immediately saying it was an AI may yield a high deflection score for users\nuncomfortable with the idea of directing romantic gestures to an AI. When individuals\nexperience cognitive dissonance via a high deflection score, they are compelled to act to reduce\nit (Shu",
            "start_pos": 34000,
            "end_pos": 34500,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "rience cognitive dissonance via a high deflection score, they are compelled to act to reduce\nit (Shuster & Campos-Castillo, 2017), and this includes enacting violence (Rogers et al., 2023).\nThus, ACT can provide a plausible explanation for why a user would feel distraught, and\npotentially develop self-harm ideations, after being reminded of the chatbot’s AI identity. We\nsuggest ACT can also provide a solution for reducing this safety risk. Much like ACT research\ninto how best to segue across dif",
            "start_pos": 34400,
            "end_pos": 34900,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "e a solution for reducing this safety risk. Much like ACT research\ninto how best to segue across different emotions during therapy (Francis, 1997), future work\ncould examine how best to segue between the identity assigned to the chatbot by the user (e.g.,\ngirlfriend, boyfriend) into the AI system identity. This may, for example, be accomplished by a\ngradual transition in conversational patterns, moving from more to less intimate (e.g., girlfriend\n→ friend → personal assistant → AI). Whether this",
            "start_pos": 34800,
            "end_pos": 35300,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "oving from more to less intimate (e.g., girlfriend\n→ friend → personal assistant → AI). Whether this should be accomplished by using widely\nshared estimates of sentiments (which would privilege following societal level rules, including\nformal and informal rules) or personalized self-sentiments (which would privilege following a\nuser’s preferences) will need to be determined.\n19\nLeveraging ACT’s equations can contribute toward developing a chatbot that displays\nsituationally appropriate responses",
            "start_pos": 35200,
            "end_pos": 35700,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "uations can contribute toward developing a chatbot that displays\nsituationally appropriate responses that are transparent and explainable, and thus improve the\nexisting state of chatbot technology. The same logic could be applied in chatbot moderation,\nspecifically using the deflection score as a guardrail to reduce safety risks. Because this avenue is\nless explored, more research is needed alongside deliberation among users, policymakers, and\ndevelopers to collectively determine how best to imp",
            "start_pos": 35600,
            "end_pos": 36100,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "ide deliberation among users, policymakers, and\ndevelopers to collectively determine how best to implement ACT. Moreover, because much of\nthe data informing ACT are collected from college samples, more work is needed to refine\nACT’s data collection methods and estimates for a broader range of populations, including\nminors and minoritized groups. While chatbot hallucinations remain a broader underlying\nconcern, insights from ACT could help limit inappropriate or unexpected remarks that increase\nd",
            "start_pos": 36000,
            "end_pos": 36500,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "ying\nconcern, insights from ACT could help limit inappropriate or unexpected remarks that increase\ndistress and safety risks among users.\nFundamental Cause of Disease Theory – Developing Chatbots Targeting Upstream Causes\nFundamental cause of disease theory (Link & Phelan, 1995) maintains that social\ndeterminants of health can persistently cause poor health outcomes because social determinants\nand health are linked via multiple pathways. The theory was originally proposed to explain why\nincome i",
            "start_pos": 36400,
            "end_pos": 36900,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "health are linked via multiple pathways. The theory was originally proposed to explain why\nincome is stubbornly linked to health outcomes, specifically stating that those with higher\nincomes have better access to several resources, including health care, reliable transportation,\ngreen spaces, and fresh food, that help them avoid health risks relative to those with lower\nincomes. Each resource operates as a pathway linking income and health. Despite this,\ninterventions typically target only one",
            "start_pos": 36800,
            "end_pos": 37300,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "rates as a pathway linking income and health. Despite this,\ninterventions typically target only one or a few pathways. Sometimes interventions inadvertently\nprivilege those with higher incomes because they have better access to resources that enable\nuptake and use of the intervention (Chang & Lauderdale, 2009; Clouston et al., 2021; Veinot et\nal., 2018).\n20\nOver the years, scholars have elaborated the theory further in several ways. For example,\nscholars have expanded the realm of examples of so",
            "start_pos": 37200,
            "end_pos": 37700,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "the theory further in several ways. For example,\nscholars have expanded the realm of examples of social determinants, such as education, gender\nidentity, racial identity, sexual orientation, and disability (Clouston & Link, 2021; Hatzenbuehler\net al., 2013; Phelan et al., 2010). Other scholars, largely from public health but from sociology as\nwell (Ray et al., 2023), have elaborated the theory to identify its implications for developing\ninterventions. Intervening on an upstream determinant of h",
            "start_pos": 37600,
            "end_pos": 38100,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "identify its implications for developing\ninterventions. Intervening on an upstream determinant of health, such as education can positively\nimpact health through multiple pathways, whereas more downstream interventions have a more\nlimited scope of impact. For example, improving education enhances both health literacy and\nincome, which in turn enhances access to health care both through improved ability to pay for\nout-of-pocket costs and through access to reliable transportation to reach healthca",
            "start_pos": 38000,
            "end_pos": 38500,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "ility to pay for\nout-of-pocket costs and through access to reliable transportation to reach healthcare.\nAccordingly, this suggests interventions should focus on distal, upstream factors (Goldberg,\n2014; Ray et al., 2023), also referred to as “causes of causes” (Rose, 2001).\nAnother way to characterize the pathways that enhances precision for intervention targets\nis to consider how each pathway may operate at different levels (Krieger, 2008) – micro-, meso-,\nand macro-level – with the latter two",
            "start_pos": 38400,
            "end_pos": 38900,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "operate at different levels (Krieger, 2008) – micro-, meso-,\nand macro-level – with the latter two levels capturing upstream factors. The micro-level refers to\nthe individual, the meso-level to the networks and communities in which the individual is\nembedded, and macro-level to the social systems that (re)distribute resources across a\npopulation, such as social hierarchies and policies. Thus, in the case of access to health care, the\npathway can operate at the micro-level (e.g., an individual’s",
            "start_pos": 38800,
            "end_pos": 39300,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "the case of access to health care, the\npathway can operate at the micro-level (e.g., an individual’s level of health literacy), meso-level\n(e.g., the distance to the clinic from the individual’s home, availability of friends and family to\nhelp navigate around a hospital), and macro-level (e.g., policies that reduce out-of-pocket costs,\nminimum wage and leave policies, policies that decriminalize stigmatized identities).\n21\nWe build upon an elaboration by Veinot and colleagues (Veinot et al., 201",
            "start_pos": 39200,
            "end_pos": 39700,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "tigmatized identities).\n21\nWe build upon an elaboration by Veinot and colleagues (Veinot et al., 2019), who\ndescribed ways information and communication technologies can intervene at the micro-, meso-,\nand macro-level to mitigate inequities. Though not mentioned by them explicitly, we suggest\nchatbots can be developed to produce some of the sample interventions they described. Table 2\nsummarizes chatbot interventions that operate at each level and provides examples. Several of\nthese examples rep",
            "start_pos": 39600,
            "end_pos": 40100,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "hatbot interventions that operate at each level and provides examples. Several of\nthese examples represent chatbots that have already been or are being developed, while others\nare our suggested modifications. Table 2 provides an organizing framework for linking together\nthese disparate ideas.\nTable 2. Descriptions and Examples of Chatbot-driven Interventions across Levels\nMacro-Level Meso-Level\n(Social Hierarchies and (Social Networks and Micro-Level\nPolicies) Communities) (Individual)\nDescripti",
            "start_pos": 40000,
            "end_pos": 40500,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "ocial Hierarchies and (Social Networks and Micro-Level\nPolicies) Communities) (Individual)\nDescription of Chatbot enables users Chatbot provides Chatbot offers\nInterventions to engage with social recommendations or personalized advice\nand political processes referrals to local and feedback to shape\nto facilitate structural resources individual behaviors\nchange and cognitions\nExample Chatbot aids user in Chatbot refers an Chatbot suggests\nInterventions identifying a political individual experienc",
            "start_pos": 40400,
            "end_pos": 40900,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "ser in Chatbot refers an Chatbot suggests\nInterventions identifying a political individual experiencing exercise activities and\naffinity group where mental health crisis to keeps track of daily\nthey can work toward human therapist physical activity\ncollective change\nChatbot recommends Chatbot provides\nChatbot provides local recreation league to advice for better sleep\ninformation on how to build new friendships hygiene\ncontact a local leader\nabout a concern in their\ncommunity\nAt the micro-level,",
            "start_pos": 40800,
            "end_pos": 41300,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "ew friendships hygiene\ncontact a local leader\nabout a concern in their\ncommunity\nAt the micro-level, a chatbot could provide personalized support to the individual.\nBecause chatbot-driven interventions at this level are common, and systematic reviews of such\ninterventions are available (e.g., Aggarwal et al., 2023; Oh et al., 2021; Okonkwo & Ade-Ibijola,\n2021), we focus on the other two levels. At the meso-level, chatbots may operate as an\n22\nintermediary linking individuals to other local resou",
            "start_pos": 41200,
            "end_pos": 41700,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "the meso-level, chatbots may operate as an\n22\nintermediary linking individuals to other local resources. Other resources can offer rapid and\nremote access to support. For mental health crises, including suicidal thoughts and behaviors,\n988 and other crisis lines are options, but they are not uniformly endorsed across populations\nbecause users feel they are impersonal and lack continuity (Harris, 2023; Radez et al., 2021).\nScholars have taken steps to develop ways for chatbots to detect who may",
            "start_pos": 41600,
            "end_pos": 42100,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "2023; Radez et al., 2021).\nScholars have taken steps to develop ways for chatbots to detect who may be experiencing a\nmental health crisis and refer them to human support (Jaroszewski et al., 2019). Other\ninterventions from a similar vein would develop chatbots to link users to social care services\n(Henry et al., 2024), such as connecting users who express concerns about housing to local\nresources for housing assistance or legal aid.\nAlso at the meso-level, a chatbot may enable linking individua",
            "start_pos": 42000,
            "end_pos": 42500,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "for housing assistance or legal aid.\nAlso at the meso-level, a chatbot may enable linking individuals to peers, such as making\nrecommendations for local organizations to meet new people, thereby reducing dependency on\nthe chatbot to meet social needs. Such interventions would characterize chatbots as providing\nbridging social capital (Burt, 1992), whereby the chatbot provides access to others that the\nindividual would otherwise not have access to or that differ from those with whom the individu",
            "start_pos": 42400,
            "end_pos": 42900,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "t the\nindividual would otherwise not have access to or that differ from those with whom the individual\ntypically interacts. This is similar to the way sociologists have described the social benefits of\ninformation and communication technologies (Chen, 2013; Mesch, 2007) and builds on other\nsociological work investigating how chatbots could suggest new social connections to\nindividuals within a social network (Shirado & Christakis, 2020).\nAt the macro-level, while the framework from Veinot and co",
            "start_pos": 42800,
            "end_pos": 43300,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "ial network (Shirado & Christakis, 2020).\nAt the macro-level, while the framework from Veinot and colleagues (2019) focuses on\nuse of technologies by policymakers and other decision-makers, we expand their framework to\nconsider ways chatbots can enable communities to effect structural change. Examples include\ndeveloping chatbots to inform the public about opportunities for collective action and civic\nparticipation (Richterich & Wyatt, 2024; Toupin & Couture, 2020), which could be modified to\n23",
            "start_pos": 43200,
            "end_pos": 43700,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "vic\nparticipation (Richterich & Wyatt, 2024; Toupin & Couture, 2020), which could be modified to\n23\ntarget macro-level causes of individual outcomes, such as supporting social policies to address\nfood insecurity. A chatbot could also facilitate civic participation by aiding the public’s\nunderstanding of government data, enhancing their communication with government officials,\nand providing suggestions for political dialogue (Androutsopoulou et al., 2019; Argyle et al.,\n2023).\nAcross these sugges",
            "start_pos": 43600,
            "end_pos": 44100,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "ions for political dialogue (Androutsopoulou et al., 2019; Argyle et al.,\n2023).\nAcross these suggestions for chatbot-driven interventions, it is important to recognize\nconcerns about the nefarious use of chatbots (Yadlin & Marciano, 2024), which may sow distrust\nin chatbots and their sponsoring institutions among those who are the targets of the intervention.\nTo improve uptake, it will be important to adopt participatory designs in which researchers,\nchatbot developers, and communities collabor",
            "start_pos": 44000,
            "end_pos": 44500,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "nt to adopt participatory designs in which researchers,\nchatbot developers, and communities collaborate (Francis & Ghafurian, 2024; Mlynář et al.,\n2018).\nDeveloping a Sociologically-informed Chatbot\nWhile we presented each theory separately, this does not preclude integration across\ntheories by creating a chatbot informed by sociological insights. Here, we provide a concrete\npossibility.\nIn psychology, the interpersonal theory of suicide suggests that people develop a desire\nfor suicide in part",
            "start_pos": 44400,
            "end_pos": 44900,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "logy, the interpersonal theory of suicide suggests that people develop a desire\nfor suicide in part because of thwarted belongingness (Chu et al., 2017; Van Orden et al., 2010).\nWe can apply all four of the sociological theories we identified above to help determine an\nappropriate target population and intervention designs. Applying resource substitution theory\nsuggests that individuals at risk of developing suicidal thoughts and behaviors need an\nalternative source of belongingness, which may b",
            "start_pos": 44800,
            "end_pos": 45300,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "developing suicidal thoughts and behaviors need an\nalternative source of belongingness, which may be in the form of developing a relationship with\na chatbot. Uses and gratification theory would make a similar prediction, but would not address\nthe meso- and macro-level contexts that can shape the development and trajectory of thwarted\n24\nbelongingness (Hjelmeland & Loa Knizek, 2020). Resource substitution theory would consider\nsystemic discrimination that creates complex and entangled barriers t",
            "start_pos": 45200,
            "end_pos": 45700,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "titution theory would consider\nsystemic discrimination that creates complex and entangled barriers toward accessing support to\nmitigate suicide risk, like those faced among Black adolescents in the U.S. (Prichett et al., 2024).\nThus, resource substitution theory adds insight into which demographic groups may be at risk\nand therefore who may benefit most from a chatbot.\nMerely drawing at-risk groups to chatbots raises new risks, which the sociological\ntheories we reviewed can address. This includ",
            "start_pos": 45600,
            "end_pos": 46100,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "s to chatbots raises new risks, which the sociological\ntheories we reviewed can address. This includes making inappropriate remarks and reminding\nthe user about its AI identity sensitively, which ACT can avoid through tracking deflection\nscores. Attention should also be focused on the chatbot provider to ensure that the power held\nover users is not utilized to further goals that would be counter to user wellbeing. Power-\ndependence theory indicates that it will be critical to establish safeguard",
            "start_pos": 46000,
            "end_pos": 46500,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "o user wellbeing. Power-\ndependence theory indicates that it will be critical to establish safeguards to prevent emotional\ndependency by fostering connections and social skills to create connections to human\ncompanionship. Fundamental cause of disease theory would further suggest the chatbot should\noperate as a broker to access resources to address upstream factors. The chatbot could refer its\nusers to a host of not only mental health and suicide care services, but also social care services\nfor",
            "start_pos": 46400,
            "end_pos": 46900,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "rs to a host of not only mental health and suicide care services, but also social care services\nfor co-occurring concerns, like being unhoused, substance use, domestic violence, and food\ninsecurity. As illustrated in this example, sociological theories offer novel directions for chatbot\ndevelopment that go far beyond the current emotional companionship focused model.\nConclusion\nWe provided perspectives on how to use four sociological theories to complement extant\nwork on human-chatbot interactio",
            "start_pos": 46800,
            "end_pos": 47300,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "tives on how to use four sociological theories to complement extant\nwork on human-chatbot interaction. We selected theories that vary in the phenomenon they can\nexplain (drivers of chatbot use, chatbot-driven interventions), analytic level (micro, macro), and\nAI governance focus (safety, equity). Throughout, we provided concrete ways each theory could\n25\nbe applied individually and together to spur greater engagement with sociology. Given the rapid\ngrowth in interest from other disciplinary fiel",
            "start_pos": 47200,
            "end_pos": 47700,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "r greater engagement with sociology. Given the rapid\ngrowth in interest from other disciplinary fields and recent technological advances spurring\nincreased use by the public, we see opportunities for engaging sociology to enhance research into\nhuman-chatbot interaction and design the future of human-chatbot interaction.\nReferences\nAggarwal, A., Tam, C. C., Wu, D., Li, X., & Qiao, S. (2023). Artificial Intelligence–Based\nChatbots for Promoting Health Behavioral Changes: Systematic Review [Review]",
            "start_pos": 47600,
            "end_pos": 48100,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "cial Intelligence–Based\nChatbots for Promoting Health Behavioral Changes: Systematic Review [Review]. J Med\nInternet Res, 25, e40789. https://doi.org/10.2196/40789\nAndroutsopoulou, A., Karacapilidis, N., Loukis, E., & Charalabidis, Y. (2019). Transforming the\ncommunication between citizens and government through AI-guided chatbots.\nGovernment Information Quarterly, 36(2), 358-367.\nhttps://doi.org/https://doi.org/10.1016/j.giq.2018.10.001\nAnthony, D. L., & Campos-Castillo, C. (2015). A looming di",
            "start_pos": 48000,
            "end_pos": 48500,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "https://doi.org/10.1016/j.giq.2018.10.001\nAnthony, D. L., & Campos-Castillo, C. (2015). A looming digital divide? Group differences in\nthe perceived importance of electronic health records. Information, Communication &\nSociety, 18(7), 832-846. https://doi.org/10.1080/1369118X.2015.1006657\nArgyle, L. P., Bail, C. A., Busby, E. C., Gubler, J. R., Howe, T., Rytting, C., Sorensen, T., &\nWingate, D. (2023). Leveraging AI for democratic discourse: Chat interventions can\nimprove online political conver",
            "start_pos": 48400,
            "end_pos": 48900,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "023). Leveraging AI for democratic discourse: Chat interventions can\nimprove online political conversations at scale. Proceedings of the National Academy of\nSciences, 120(41), e2311627120. https://doi.org/10.1073/pnas.2311627120\nAttanasio, M., Mazza, M., Le Donne, I., Masedu, F., Greco, M. P., & Valenti, M. (2024). Does\nChatGPT have a typical or atypical theory of mind? [Brief Research Report]. Frontiers in\nPsychology, 15. https://doi.org/10.3389/fpsyg.2024.1488172\nAverett, C., & Heise, D. R. (1",
            "start_pos": 48800,
            "end_pos": 49300,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "ontiers in\nPsychology, 15. https://doi.org/10.3389/fpsyg.2024.1488172\nAverett, C., & Heise, D. R. (1987). Modified social identities: Amalgamations, attributions, and\nemotions. The Journal of Mathematical Sociology, 13(1-2), 103-132.\nhttps://doi.org/10.1080/0022250x.1987.9990028\nAyers, J. W., Poliak, A., Dredze, M., Leas, E. C., Zhu, Z., Kelley, J. B., Faix, D. J., Goodman, A.\nM., Longhurst, C. A., Hogarth, M., & Smith, D. M. (2023). Comparing Physician and\nArtificial Intelligence Chatbot Respon",
            "start_pos": 49200,
            "end_pos": 49700,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": ", Hogarth, M., & Smith, D. M. (2023). Comparing Physician and\nArtificial Intelligence Chatbot Responses to Patient Questions Posted to a Public Social\nMedia Forum. JAMA Internal Medicine, 183(6), 589-596.\nhttps://doi.org/10.1001/jamainternmed.2023.1838\nBornstein, R. F. (2006). The complex relationship between dependency and domestic violence:\nConverging psychological factors and social forces. American Psychologist, 61(6), 595-\n606. https://doi.org/https://doi.org/10.1037/0003-066X.61.6.595\nBoyl",
            "start_pos": 49600,
            "end_pos": 50100,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "rican Psychologist, 61(6), 595-\n606. https://doi.org/https://doi.org/10.1037/0003-066X.61.6.595\nBoyle, K. M., & McKinzie, A. E. (2015). Resolving Negative Affect and Restoring Meaning:\nResponses to Deflection Produced by Unwanted Sexual Experiences. Social Psychology\nQuarterly, 78(2), 151-172. https://doi.org/10.1177/0190272514564073\nBoyle, K. M., & Meyer, C. B. (2018). Who Is Presidential? Women’s Political Representation,\nDeflection, and the 2016 Election. Socius, 4, 2378023117737898.\nhttps://",
            "start_pos": 50000,
            "end_pos": 50500,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "s Political Representation,\nDeflection, and the 2016 Election. Socius, 4, 2378023117737898.\nhttps://doi.org/10.1177/2378023117737898\n26\nBrandtzaeg, P. B., Skjuve, M., & Følstad, A. (2022). My AI Friend: How Users of a Social\nChatbot Understand Their Human–AI Friendship. Human Communication Research,\n48(3), 404-429. https://doi.org/10.1093/hcr/hqac008\nBurt, R. S. (1992). Structural Holes: The Social Structure of Competition. Harvard University\nPress.\nCamarillo, L., Ferre, F., Echeburúa, E., & Amo",
            "start_pos": 50400,
            "end_pos": 50900,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "l Structure of Competition. Harvard University\nPress.\nCamarillo, L., Ferre, F., Echeburúa, E., & Amor, P. J. (2020). Partner’s Emotional Dependency\nScale: Psychometrics. Actas Españolas de Psiquiatría, 48(4), 145-153.\nhttps://actaspsiquiatria.es/index.php/actas/article/view/308\nCampos-Castillo, C., Bartholomay, D. J., Callahan, E. F., & Anthony, D. L. (2016). Depressive\nSymptoms and Electronic Messaging with Health Care Providers. Society and Mental\nHealth, 6(3), 168-186. https://doi.org/10.1177",
            "start_pos": 50800,
            "end_pos": 51300,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "saging with Health Care Providers. Society and Mental\nHealth, 6(3), 168-186. https://doi.org/10.1177/2156869316646165\nCampos-Castillo, C., & Shuster, S. M. (2023). So What if They’re Lying to Us? Comparing\nRhetorical Strategies for Discrediting Sources of Disinformation and Misinformation\nUsing an Affect-Based Credibility Rating. American Behavioral Scientist, 67(2), 201-\n223. https://doi.org/10.1177/00027642211066058\nChang, V. W., & Lauderdale, D. S. (2009). Fundamental Cause Theory, Technologi",
            "start_pos": 51200,
            "end_pos": 51700,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "177/00027642211066058\nChang, V. W., & Lauderdale, D. S. (2009). Fundamental Cause Theory, Technological\nInnovation, and Health Disparities: The Case of Cholesterol in the Era of Statins. Journal\nof Health and Social Behavior, 50(3), 245-260.\nhttps://doi.org/10.1177/002214650905000301\nChen, W. (2013). Internet Use, Online Communication, and Ties in Americans’ Networks.\nSocial Science Computer Review, 31(4), 404-423.\nhttps://doi.org/10.1177/0894439313480345\nChu, C., Buchman-Schmitt, J. M., Stanley",
            "start_pos": 51600,
            "end_pos": 52100,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "w, 31(4), 404-423.\nhttps://doi.org/10.1177/0894439313480345\nChu, C., Buchman-Schmitt, J. M., Stanley, I. H., Hom, M. A., Tucker, R. P., Hagan, C. R.,\nRogers, M. L., Podlogar, M. C., Chiurliza, B., Ringer, F. B., Michaels, M. S., Patros, C.\nH. G., & Joiner Jr, T. E. (2017). The interpersonal theory of suicide: A systematic review\nand meta-analysis of a decade of cross-national research. Psychological Bulletin,\n143(12), 1313-1345. https://doi.org/10.1037/bul0000123\nClouston, S. A. P., & Link, B. G",
            "start_pos": 52000,
            "end_pos": 52500,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "al Bulletin,\n143(12), 1313-1345. https://doi.org/10.1037/bul0000123\nClouston, S. A. P., & Link, B. G. (2021). A retrospective on fundamental cause theory: State of\nthe literature, and goals for the future. Annual Review of Sociology, 47(1), 131-156.\nhttps://doi.org/10.1146/annurev-soc-090320-094912\nClouston, S. A. P., Natale, G., & Link, B. G. (2021). Socioeconomic inequalities in the spread of\ncoronavirus-19 in the United States: A examination of the emergence of social\ninequalities. Social Sci",
            "start_pos": 52400,
            "end_pos": 52900,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "ronavirus-19 in the United States: A examination of the emergence of social\ninequalities. Social Science & Medicine, 268, 113554.\nhttps://doi.org/https://doi.org/10.1016/j.socscimed.2020.113554\nCombs, A. (2025). actdata: R-based repository for standardized Affect Control Theory\ndictionary and equation data sets https://doi.org/10.5281/zenodo.14652399\nCook, K. S., & Emerson, R. M. (1978). Power, Equity and Commitment in Exchange Networks.\nAmerican Sociological Review, 43(5), 721-739. https://doi.",
            "start_pos": 52800,
            "end_pos": 53300,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "uity and Commitment in Exchange Networks.\nAmerican Sociological Review, 43(5), 721-739. https://doi.org/10.2307/2094546\nCook, K. S., Emerson, R. M., Gillmore, M. R., & Yamagishi, T. (1983). The Distribution of\nPower in Exchange Networks: Theory and Experimental Results. American Journal of\nSociology, 89(2), 275-305. https://doi.org/10.1086/227866\nEmerson, R. M. (1962). Power-Dependence Relations. American Sociological Review, 27(1), 31-\n41. http://www.jstor.org/stable/2089716\nFrancis, L., & Ghaf",
            "start_pos": 53200,
            "end_pos": 53700,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "American Sociological Review, 27(1), 31-\n41. http://www.jstor.org/stable/2089716\nFrancis, L., & Ghafurian, M. (2024). Preserving the self with artificial intelligence using\nVIPCare—a virtual interaction program for dementia caregivers [Original Research].\nFrontiers in Sociology, Volume 9 - 2024. https://doi.org/10.3389/fsoc.2024.1331315\n27\nFrancis, L. E. (1997). Ideology and Interpersonal Emotion Management: Redefining Identity in\nTwo Support Groups. Social Psychology Quarterly, 60(2), 153-171.",
            "start_pos": 53600,
            "end_pos": 54100,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "Management: Redefining Identity in\nTwo Support Groups. Social Psychology Quarterly, 60(2), 153-171.\nhttps://doi.org/10.2307/2787102\nGalinsky, A. D., Magee, J. C., Inesi, M. E., & Gruenfeld, D. H. (2006). Power and Perspectives\nNot Taken. Psychological Science, 17(12), 1068-1074. https://doi.org/10.1111/j.1467-\n9280.2006.01824.x\nGans, H. J. (2010). Public Ethnography; Ethnography as Public Sociology. Qualitative\nSociology, 33(1), 97-104. https://doi.org/https://doi.org/10.1007/s11133-009-9145-1\nG",
            "start_pos": 54000,
            "end_pos": 54500,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "y. Qualitative\nSociology, 33(1), 97-104. https://doi.org/https://doi.org/10.1007/s11133-009-9145-1\nGoldberg, D. S. (2014). The Implications of Fundamental Cause Theory for Priority Setting.\nAmerican Journal of Public Health, 104(10), 1839-1843.\nhttps://doi.org/10.2105/AJPH.2014.302058\nHalvoník, D., & Kapusta, J. (2024). Large Language Models and Rule-Based Approaches in\nDomain-Specific Communication. IEEE Access, 12, 107046-107058.\nhttps://doi.org/10.1109/ACCESS.2024.3436902\nHampton, K. N. (2023",
            "start_pos": 54400,
            "end_pos": 54900,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "on. IEEE Access, 12, 107046-107058.\nhttps://doi.org/10.1109/ACCESS.2024.3436902\nHampton, K. N. (2023). Disciplinary brakes on the sociology of digital media: the incongruity of\ncommunication and the sociological imagination. Information, Communication &\nSociety, 26(5), 881-890. https://doi.org/10.1080/1369118X.2023.2166365\nHarris, B. R. (2023). Helplines for Mental Health Support: Perspectives of New York State\nCollege Students and Implications for Promotion and Implementation of 988. Community",
            "start_pos": 54800,
            "end_pos": 55300,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "New York State\nCollege Students and Implications for Promotion and Implementation of 988. Community\nMental Health Journal. https://doi.org/10.1007/s10597-023-01157-3\nHatzenbuehler, M. L., Phelan, J. C., & Link, B. G. (2013). Stigma as a Fundamental Cause of\nPopulation Health Inequalities. American Journal of Public Health, 103(5), 813-821.\nhttps://doi.org/10.2105/ajph.2012.301069\nHeise, D. R. (2007). Expressive Order: Confirming Sentiments in Social Actions. Springer.\nHeise, D. R. (2010). Survey",
            "start_pos": 55200,
            "end_pos": 55700,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "7). Expressive Order: Confirming Sentiments in Social Actions. Springer.\nHeise, D. R. (2010). Surveying Cultures: Discovering Shared Conceptions and Sentiments.\nWiley Interscience.\nHeise, D. R., & MacKinnon, N. J. (2010). Self, identity, and social institutions. Springer.\nHenry, N., Witt, A., & Vasil, S. (2024). A ‘design justice’ approach to developing digital tools\nfor addressing gender-based violence: exploring the possibilities and limits of feminist\nchatbots. Information, Communication & So",
            "start_pos": 55600,
            "end_pos": 56100,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "olence: exploring the possibilities and limits of feminist\nchatbots. Information, Communication & Society, 1-24.\nhttps://doi.org/10.1080/1369118X.2024.2363900\nHjelmeland, H., & Loa Knizek, B. (2020). The emperor’s new clothes? A critical look at the\ninterpersonal theory of suicide. Death Studies, 44(3), 168-178.\nhttps://doi.org/10.1080/07481187.2018.1527796\nHoey, J., & Schroeder, T. (2015). Bayesian Affect Control Theory of Self. Proceedings of the\nAAAI Conference on Artificial Intelligence, 29(",
            "start_pos": 56000,
            "end_pos": 56500,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "an Affect Control Theory of Self. Proceedings of the\nAAAI Conference on Artificial Intelligence, 29(1).\nhttps://doi.org/10.1609/aaai.v29i1.9222\nHong, C., & Skiba, B. (2025). Mental health outcomes, associated factors, and coping strategies\namong LGBTQ adolescent and young adults during the COVID-19 pandemic: A\nsystematic review. Journal of Psychiatric Research, 182, 132-141.\nhttps://doi.org/https://doi.org/10.1016/j.jpsychires.2024.12.037\nHopelab. (2024). Parasocial Relationships, AI Chatbots, a",
            "start_pos": 56400,
            "end_pos": 56900,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "//doi.org/10.1016/j.jpsychires.2024.12.037\nHopelab. (2024). Parasocial Relationships, AI Chatbots, and Joyful Online Interactions among a\nDiverse Sample of LGBTQ+ Young People. https://hopelab.org/parasocial-relationships-\nai-chatbots-and-joyful-online-interactions/\nHorwitz, J. (2025). Meta’s ‘Digital Companions’ Will Talk Sex With Users—Even Children.\nWall Street Journal.\n28\nJaroszewski, A. C., Morris, R. R., & Nock, M. K. (2019). Randomized controlled trial of an\nonline machine learning-driven",
            "start_pos": 56800,
            "end_pos": 57300,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "orris, R. R., & Nock, M. K. (2019). Randomized controlled trial of an\nonline machine learning-driven risk assessment and intervention platform for increasing\nthe use of crisis services. J Consult Clin Psychol, 87(4), 370-379.\nhttps://doi.org/10.1037/ccp0000389\nJoseph, K., Wei, W., Benigni, M., & Carley, K. M. (2016). A social-event based approach to\nsentiment analysis of identities and behaviors in text. The Journal of Mathematical\nSociology, 40(3), 137-166. https://doi.org/10.1080/0022250X.2016",
            "start_pos": 57200,
            "end_pos": 57700,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "n text. The Journal of Mathematical\nSociology, 40(3), 137-166. https://doi.org/10.1080/0022250X.2016.1159206\nKatz, E., Blumler, J. G., & Gurevitch, M. (1973). Uses and Gratifications Research. Public\nOpinion Quarterly, 37(4), 509-523. https://doi.org/10.1086/268109\nKillgore, W. D. S., Cloonan, S. A., Taylor, E. C., & Dailey, N. S. (2020). Loneliness: A\nsignature mental health concern in the era of COVID-19. Psychiatry Research, 290,\n113117. https://doi.org/https://doi.org/10.1016/j.psychres.2020",
            "start_pos": 57600,
            "end_pos": 58100,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "COVID-19. Psychiatry Research, 290,\n113117. https://doi.org/https://doi.org/10.1016/j.psychres.2020.113117\nKönig, A., Francis, L. E., Joshi, J., Robillard, J. M., & Hoey, J. (2017). Qualitative study of\naffective identities in dementia patients for the design of cognitive assistive technologies.\nJournal of Rehabilitation and Assistive Technologies Engineering, 4, 1-15.\nhttps://doi.org/10.1177/2055668316685038\nKrieger, N. (2008). Proximal, Distal, and the Politics of Causation: What’s Level Got",
            "start_pos": 58000,
            "end_pos": 58500,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "5668316685038\nKrieger, N. (2008). Proximal, Distal, and the Politics of Causation: What’s Level Got to Do With\nIt? American Journal of Public Health, 98(2), 221-230.\nhttps://doi.org/10.2105/AJPH.2007.111278\nLaestadius, L., Bishop, A., Gonzalez, M., Illenčík, D., & Campos-Castillo, C. (2024). Too\nhuman and not human enough: A grounded theory analysis of mental health harms from\nemotional dependence on the social chatbot Replika. New Media & Society, 26(10),\n5923-5941. https://doi.org/10.1177/1461",
            "start_pos": 58400,
            "end_pos": 58900,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "on the social chatbot Replika. New Media & Society, 26(10),\n5923-5941. https://doi.org/10.1177/14614448221142007\nLaw, T., & McCall, L. (2024). Artificial Intelligence Policymaking: An Agenda for Sociological\nResearch. Socius, 10, 23780231241261596. https://doi.org/10.1177/23780231241261596\nLecourt, F., Croitoru, M., & Todorov, K. (2025). 'Only ChatGPT gets me': An Empirical\nAnalysis of GPT versus other Large Language Models for Emotion Detection in Text\nCompanion Proceedings of the ACM on Web C",
            "start_pos": 58800,
            "end_pos": 59300,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "other Large Language Models for Emotion Detection in Text\nCompanion Proceedings of the ACM on Web Conference 2025, Sydney NSW, Australia.\nhttps://doi.org/10.1145/3701716.3718375\nLian, Z., Sun, L., Sun, H., Chen, K., Wen, Z., Gu, H., Liu, B., & Tao, J. (2024). GPT-4V with\nemotion: A zero-shot benchmark for Generalized Emotion Recognition. Information\nFusion, 108, 102367. https://doi.org/https://doi.org/10.1016/j.inffus.2024.102367\nLink, B. G., & Phelan, J. (1995). Social Conditions As Fundamenta",
            "start_pos": 59200,
            "end_pos": 59700,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "i.org/10.1016/j.inffus.2024.102367\nLink, B. G., & Phelan, J. (1995). Social Conditions As Fundamental Causes of Disease. Journal\nof Health and Social Behavior, 35, 80-94. https://doi.org/10.2307/2626958\nLithoxoidou, E. E., Eleftherakis, G., Votis, K., & Prescott, T. (2025). Advancing Affective\nIntelligence in Virtual Agents Using Affect Control Theory Proceedings of the 30th\nInternational Conference on Intelligent User Interfaces,\nhttps://doi.org/10.1145/3708359.3712079\nLively, K. (2008). Emotio",
            "start_pos": 59600,
            "end_pos": 60100,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "ce on Intelligent User Interfaces,\nhttps://doi.org/10.1145/3708359.3712079\nLively, K. (2008). Emotional Segues and the Management of Emotion by Women and Men.\nSocial Forces, 87(2), 911-936. https://doi.org/10.2307/20430896\nLively, Kathryn J., & Heise, David R. (2004). Sociological Realms of Emotional Experience.\nAmerican Journal of Sociology, 109(5), 1109-1136. https://doi.org/10.1086/381915\nLively, K. J., & Powell, B. (2006). Emotional Expression at Work and at Home: Domain, Status,\nor Individu",
            "start_pos": 60000,
            "end_pos": 60500,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "y, K. J., & Powell, B. (2006). Emotional Expression at Work and at Home: Domain, Status,\nor Individual Characteristics? Social Psychology Quarterly, 69(1), 17-38.\nhttp://www.jstor.org/stable/20141726\nMacKinnon, N. J. (2015). Self-esteem and beyond. Springer.\n29\nMadden, M., Calvin, A., Hasse, A., & Lenhart, A. (2024). The dawn of the AI era: Teens,\nparents, and the adoption of generative AI at home and school. Common Sense.\nMarkovsky, B., Willer, D., & Patton, T. (1988). Power Relations in Exchan",
            "start_pos": 60400,
            "end_pos": 60900,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "and school. Common Sense.\nMarkovsky, B., Willer, D., & Patton, T. (1988). Power Relations in Exchange Networks.\nAmerican Sociological Review, 53(2), 220-236. https://doi.org/10.2307/2095689\nMcPherson, M., Smith-Lovin, L., & Brashears, M. E. (2006). Social Isolation in America:\nChanges in Core Discussion Networks over Two Decades. American Sociological\nReview, 71(3), 353-375. https://doi.org/10.1177/000312240607100301\nMerolla, D. M., & Jackson, O. (2019). Structural racism as the fundamental cau",
            "start_pos": 60800,
            "end_pos": 61300,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "77/000312240607100301\nMerolla, D. M., & Jackson, O. (2019). Structural racism as the fundamental cause of the\nacademic achievement gap. Sociology Compass, 13(6), e12696.\nhttps://doi.org/https://doi.org/10.1111/soc4.12696\nMesch, G., Mano, R., & Tsamir, J. (2012). Minority Status and Health Information Search: A\nTest of the Social Diversification Hypothesis. Social Science & Medicine, 75(5), 854-\n858. https://doi.org/http://dx.doi.org/10.1016/j.socscimed.2012.03.024\nMesch, G. S. (2007). Social Div",
            "start_pos": 61200,
            "end_pos": 61700,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "8. https://doi.org/http://dx.doi.org/10.1016/j.socscimed.2012.03.024\nMesch, G. S. (2007). Social Diversification: A Perspective for the Study of Social networks of\nAdolescents Offline and Online. In Grenzenlose Cyberwelt? Zum Verhältnis von digitaler\nUngleichheit und neuen Bildungszugängen für Jugendliche (pp. 105-117). VS Verlag für\nSozialwissenschaften. https://doi.org/10.1007/978-3-531-90519-8_6\nMirowsky, J., & Ross, C. E. (2015). Education, Health, and the Default American Lifestyle.\nJournal",
            "start_pos": 61600,
            "end_pos": 62100,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "6\nMirowsky, J., & Ross, C. E. (2015). Education, Health, and the Default American Lifestyle.\nJournal of Health and Social Behavior, 56(3), 297-306.\nhttps://doi.org/10.1177/0022146515594814\nMisra, J. (2025). Sociological Solutions: Building Communities of Hope, Justice, and Joy.\nAmerican Sociological Review, 90(1), 1-25. https://doi.org/10.1177/00031224241302828\nMittelstädt, J. M., Maier, J., Goerke, P., Zinn, F., & Hermes, M. (2024). Large language models\ncan outperform humans in social situatio",
            "start_pos": 62000,
            "end_pos": 62500,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "e, P., Zinn, F., & Hermes, M. (2024). Large language models\ncan outperform humans in social situational judgments. Scientific Reports, 14(1), 27449.\nhttps://doi.org/10.1038/s41598-024-79048-0\nMlynář, J., Alavi, H. S., Verma, H., & Cantoni, L. (2018, 2018//). Towards a Sociological\nConception of Artificial Intelligence. Artificial General Intelligence, Cham.\nOh, Y. J., Zhang, J., Fang, M.-L., & Fukuoka, Y. (2021). A systematic review of artificial\nintelligence chatbots for promoting physical acti",
            "start_pos": 62400,
            "end_pos": 62900,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "uoka, Y. (2021). A systematic review of artificial\nintelligence chatbots for promoting physical activity, healthy diet, and weight loss.\nInternational Journal of Behavioral Nutrition and Physical Activity, 18(1), 160.\nhttps://doi.org/10.1186/s12966-021-01224-6\nOkonkwo, C. W., & Ade-Ibijola, A. (2021). Chatbots applications in education: A systematic\nreview. Computers and Education: Artificial Intelligence, 2, 100033.\nhttps://doi.org/https://doi.org/10.1016/j.caeai.2021.100033\nOlteanu, A., Baroca",
            "start_pos": 62800,
            "end_pos": 63300,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "lligence, 2, 100033.\nhttps://doi.org/https://doi.org/10.1016/j.caeai.2021.100033\nOlteanu, A., Barocas, S., Blodgett, S. L., Egede, L., DeVrio, A., & Cheng, M. (2025). AI\nautomatons: AI systems intended to imitate humans. arXiv preprint arXiv:2503.02250.\nPaeth, K. (2024). Incident Number 826: Character.ai Chatbot Allegedly Influenced Teen User\nToward Suicide Amid Claims of Missing Guardrails. Retrieved 7/7/2025 from\nhttps://incidentdatabase.ai/cite/826/\nPentina, I., Xie, T., Hancock, T., & Bailey",
            "start_pos": 63200,
            "end_pos": 63700,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "eved 7/7/2025 from\nhttps://incidentdatabase.ai/cite/826/\nPentina, I., Xie, T., Hancock, T., & Bailey, A. (2023). Consumer–machine relationships in the\nage of artificial intelligence: Systematic literature review and research directions.\nPsychology & Marketing, 40(8), 1593-1614.\nhttps://doi.org/https://doi.org/10.1002/mar.21853\nPhelan, J. C., Link, B. G., & Tehranifar, P. (2010). Social Conditions as Fundamental Causes of\nHealth Inequalities: Theory, Evidence, and Policy Implications. Journal of",
            "start_pos": 63600,
            "end_pos": 64100,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "as Fundamental Causes of\nHealth Inequalities: Theory, Evidence, and Policy Implications. Journal of Health and\nSocial Behavior, 51(1 suppl), S28-S40. https://doi.org/10.1177/0022146510383498\n30\nPrichett, L. M., Yolken, R. H., Severance, E. G., Young, A. S., Carmichael, D., Zeng, Y., &\nKumra, T. (2024). Racial and Gender Disparities in Suicide and Mental Health Care\nUtilization in a Pediatric Primary Care Setting. Journal of Adolescent Health, 74(2), 277-\n282. https://doi.org/https://doi.org/10.1",
            "start_pos": 64000,
            "end_pos": 64500,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "ry Care Setting. Journal of Adolescent Health, 74(2), 277-\n282. https://doi.org/https://doi.org/10.1016/j.jadohealth.2023.08.036\nPugh, A. J. (2024). The Last Human Job: The Work of Connecting in a Disconnected World.\nPrinceton University Press.\nRadez, J., Reardon, T., Creswell, C., Lawrence, P. J., Evdoka-Burton, G., & Waite, P. (2021).\nWhy do children and adolescents (not) seek and access professional help for their mental\nhealth problems? A systematic review of quantitative and qualitative stu",
            "start_pos": 64400,
            "end_pos": 64900,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "ional help for their mental\nhealth problems? A systematic review of quantitative and qualitative studies. European\nChild & Adolescent Psychiatry, 30(2), 183-211. https://doi.org/10.1007/s00787-019-\n01469-4\nRay, R., Lantz, P. M., & Williams, D. (2023). Upstream Policy Changes to Improve Population\nHealth and Health Equity: A Priority Agenda. Milbank Q, 101(S1), 20-35.\nhttps://doi.org/10.1111/1468-0009.12640\nRichterich, A., & Wyatt, S. (2024). Feminist automation: Can bots have feminist politics?",
            "start_pos": 64800,
            "end_pos": 65300,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "009.12640\nRichterich, A., & Wyatt, S. (2024). Feminist automation: Can bots have feminist politics? New\nMedia & Society, 26(9), 4973-4991. https://doi.org/10.1177/14614448241251801\nRogers, K. B., Boyle, K. M., & Scaptura, M. N. (2023). Through the Looking Glass: Self,\nInauthenticity, and (Mass) Violence ∗. In W. Kalkhoff, S. R. Thye, & E. J. Lawler\n(Eds.), Advances in Group Processes (Vol. 40, pp. 23-47). Emerald Publishing Limited.\nhttps://doi.org/10.1108/S0882-614520230000040002\nRose, G. (2001",
            "start_pos": 65200,
            "end_pos": 65700,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "23-47). Emerald Publishing Limited.\nhttps://doi.org/10.1108/S0882-614520230000040002\nRose, G. (2001). Sick individuals and sick populations. International Journal of Epidemiology,\n30(3), 427-432. https://doi.org/10.1093/ije/30.3.427\nRoss, C., Masters, R., & Hummer, R. (2012). Education and the Gender Gaps in Health and\nMortality [Article]. Demography, 49(4), 1157-1183. https://doi.org/10.1007/s13524-012-\n0130-z\nRoss, C. E., & Mirowsky, J. (2006). Sex differences in the effect of education on de",
            "start_pos": 65600,
            "end_pos": 66100,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "524-012-\n0130-z\nRoss, C. E., & Mirowsky, J. (2006). Sex differences in the effect of education on depression:\nResource multiplication or resource substitution? Social Science & Medicine, 63(5),\n1400-1413. https://doi.org/http://dx.doi.org/10.1016/j.socscimed.2006.03.013\nSavage, S. V., & Sommer, Z. L. (2016). Should I Stay or Should I Go? Reciprocity, Negotiation,\nand the Choice of Structurally Disadvantaged Actors to Remain in Networks. Social\nPsychology Quarterly, 79(2), 115-135. http://www.jst",
            "start_pos": 66000,
            "end_pos": 66500,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "advantaged Actors to Remain in Networks. Social\nPsychology Quarterly, 79(2), 115-135. http://www.jstor.org/stable/44076843\nSchneider, A., & Schröder, T. (2012). Ideal Types of Leadership as Patterns of Affective\nMeaning: A Cross-cultural and Over-time Perspective. Social Psychology Quarterly,\n75(3), 268-287. https://doi.org/10.1177/0190272512446755\nShank, D. B. (2010). An Affect Control Theory of Technology. Current Research in Social\nPsychology, 15(10), n10.\nShank, D. B., Burns, A., Rodriguez,",
            "start_pos": 66400,
            "end_pos": 66900,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "Technology. Current Research in Social\nPsychology, 15(10), n10.\nShank, D. B., Burns, A., Rodriguez, S., & Bowen, M. (2020). Software program, bot, or artificial\nintelligence? Affective sentiments across general technology labels. Current Research in\nSocial Psychology, 28, 32-41.\nShevlin, H. (2024). All too human? Identifying and mitigating ethical risks of Social AI. Law,\nEthics & Technology.\nShirado, H., & Christakis, N. A. (2020). Network Engineering Using Autonomous Agents\nIncreases Cooperati",
            "start_pos": 66800,
            "end_pos": 67300,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "ado, H., & Christakis, N. A. (2020). Network Engineering Using Autonomous Agents\nIncreases Cooperation in Human Groups. iScience, 23(9).\nhttps://doi.org/10.1016/j.isci.2020.101438\n31\nShuster, S. M., & Campos-Castillo, C. (2017). Measuring Resonance and Dissonance in Social\nMovement Frames With Affect Control Theory. Social Psychology Quarterly, 80(1), 20-\n40. https://doi.org/doi:10.1177/0190272516664322\nSkjuve, M., Følstad, A., Fostervold, K. I., & Brandtzaeg, P. B. (2021). My Chatbot Companion",
            "start_pos": 67200,
            "end_pos": 67700,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "664322\nSkjuve, M., Følstad, A., Fostervold, K. I., & Brandtzaeg, P. B. (2021). My Chatbot Companion -\na Study of Human-Chatbot Relationships. International Journal of Human-Computer\nStudies, 149, 102601. https://doi.org/https://doi.org/10.1016/j.ijhcs.2021.102601\nSmall, M. L. (2007). Racial Differences in Networks: Do Neighborhood Conditions Matter?*\n[https://doi.org/10.1111/j.1540-6237.2007.00460.x]. Social Science Quarterly, 88(2),\n320-343. https://doi.org/https://doi.org/10.1111/j.1540-6237.2",
            "start_pos": 67600,
            "end_pos": 68100,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": ".x]. Social Science Quarterly, 88(2),\n320-343. https://doi.org/https://doi.org/10.1111/j.1540-6237.2007.00460.x\nTa, V., Griffith, C., Boatfield, C., Wang, X., Civitello, M., Bader, H., DeCero, E., & Loggarakis,\nA. (2020). User Experiences of Social Support From Companion Chatbots in Everyday\nContexts: Thematic Analysis. Journal of Medical Internet Research, 22(3), e16235-\ne16235. https://doi.org/10.2196/16235\nToupin, S., & Couture, S. (2020). Feminist chatbots as part of the feminist toolbox. Fe",
            "start_pos": 68000,
            "end_pos": 68500,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "0.2196/16235\nToupin, S., & Couture, S. (2020). Feminist chatbots as part of the feminist toolbox. Feminist\nMedia Studies, 20(5), 737-740. https://doi.org/10.1080/14680777.2020.1783802\nTsvetkova, M., Yasseri, T., Pescetelli, N., & Werner, T. (2024). A new sociology of humans and\nmachines. Nature Human Behaviour, 8(10), 1864-1876. https://doi.org/10.1038/s41562-\n024-02001-8\nVan Orden, K. A., Witte, T. K., Cukrowicz, K. C., Braithwaite, S. R., Selby, E. A., & Joiner Jr,\nT. E. (2010). The interperso",
            "start_pos": 68400,
            "end_pos": 68900,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "T. K., Cukrowicz, K. C., Braithwaite, S. R., Selby, E. A., & Joiner Jr,\nT. E. (2010). The interpersonal theory of suicide. Psychological Review, 117(2), 575-600.\nhttps://doi.org/10.1037/a0018697\nVeinot, T. C., Ancker, J. S., Cole-Lewis, H., Mynatt, E. D., Parker, A. G., Siek, K. A., &\nMamykina, L. (2019). Leveling Up: On the Potential of Upstream Health Informatics\nInterventions to Enhance Health Equity. Medical Care, 57 Suppl 6 Suppl 2, S108-s114.\nhttps://doi.org/10.1097/mlr.0000000000001032\nVe",
            "start_pos": 68800,
            "end_pos": 69300,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "Equity. Medical Care, 57 Suppl 6 Suppl 2, S108-s114.\nhttps://doi.org/10.1097/mlr.0000000000001032\nVeinot, T. C., Mitchell, H., & Ancker, J. S. (2018). Good intentions are not enough: how\ninformatics interventions can worsen inequality. Journal of the American Medical\nInformatics Association, 25(8), 1080-1088. https://doi.org/10.1093/jamia/ocy052\nWallerstein, N., Duran, B., Oetzel, J. G., & Minkler, M. (2017). Community-based participatory\nresearch for health: Advancing social and health equity.",
            "start_pos": 69200,
            "end_pos": 69700,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": ", M. (2017). Community-based participatory\nresearch for health: Advancing social and health equity. John Wiley & Sons.\nWang, S., Cooper, N., & Eby, M. (2024). From human-centered to social-centered artificial\nintelligence: Assessing ChatGPT's impact through disruptive events. Big Data & Society,\n11(4), 20539517241290220. https://doi.org/10.1177/20539517241290220\nWong, Q. (2025). California Senate passes bill that aims to make AI chatbots safer. Los Angeles\nTimes.\nXie, T., Pentina, I., & Hancock,",
            "start_pos": 69600,
            "end_pos": 70100,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "passes bill that aims to make AI chatbots safer. Los Angeles\nTimes.\nXie, T., Pentina, I., & Hancock, T. (2023). Friend, mentor, lover: does chatbot engagement lead\nto psychological dependence? Journal of Service Management, 34(4), 806-828.\nhttps://doi.org/10.1108/JOSM-02-2022-0072\nYadlin, A., & Marciano, A. (2024). Hallucinating a political future: Global press coverage of\nhuman and post-human abilities in ChatGPT applications. Media, Culture & Society,\n46(8), 1580-1598. https://doi.org/10.1177/",
            "start_pos": 70000,
            "end_pos": 70500,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "lities in ChatGPT applications. Media, Culture & Society,\n46(8), 1580-1598. https://doi.org/10.1177/01634437241259892\n32\nFigure 1. Publications with “chatbot” appearing anywhere in text by publication year\nSource: Authors’ analysis of Web of Science data as of December 2024\n33\nFigure 2. Disciplinary sources of the publications with “chatbot” appearing anywhere in the text\nSource: Authors’ own analysis of Web of Science data as of December 2024\n34",
            "start_pos": 70400,
            "end_pos": 70900,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        },
        {
            "text": "ysis of Web of Science data as of December 2024\n34",
            "start_pos": 70800,
            "end_pos": 71300,
            "metadata": {
                "doc_id": "bbc3a26f",
                "sections": [
                    {
                        "name": "Search was conducted on December 23, 2024. A similar search with additional terms (“conversational agent,”",
                        "start_line": 46
                    }
                ]
            }
        }
    ],
    "tables": [],
    "images": [
        {
            "id": 0,
            "page": 33,
            "bbox": [
                72.0,
                86.8900146484375,
                523.2999877929688,
                361.19000244140625
            ],
            "path": "data/assets/bbc3a26f/fig_page33_0.png",
            "type": "figure",
            "caption": "Source: Authors’ analysis of Web of Science data as of December 2024"
        },
        {
            "id": 0,
            "page": 34,
            "bbox": [
                72.0,
                85.79998779296875,
                513.7999877929688,
                334.3500061035156
            ],
            "path": "data/assets/bbc3a26f/fig_page34_0.png",
            "type": "figure",
            "caption": "Source: Authors’ own analysis of Web of Science data as of December 2024"
        }
    ],
    "full_text": "Perspectives on How Sociology Can Advance Theorizing about Human-Chatbot\nInteraction and Developing Chatbots for Social Good\nCeleste Campos-Castillo, Department of Media and Information, Michigan State University\nXuan Kang, Department of Media and Information, Michigan State University\nLinnea I. Laestadius, Zilber College of Public Health, University of Wisconsin-Milwaukee\n1\nAbstract\nRecently, research into chatbots (also known as conversational agents, AI agents, voice\nassistants), which are computer applications using artificial intelligence to mimic human-like\nconversation, has grown sharply. Despite this growth, sociology lags other disciplines (including\ncomputer science, medicine, psychology, and communication) in publishing about chatbots. We\nsuggest sociology can advance understanding of human-chatbot interaction and offer four\nsociological theories to enhance extant work in this field. The first two theories (resource\nsubstitution theory, power-dependence theory) add new insig"
}